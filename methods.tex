\section{Materials and Methods}
\label{sec:method}
\paragraph{Notation.} 
Consider a locus with starting derived allele frequency
$\nu_0$. Frequencies are sampled at $T+1$ distinct generations specified
by ${\cal T}= \{\tau_i: 0\le \tau_0<\tau_1,\ldots\le \tau_T\}$, and
denoted by $\bm{\nu}=\{\nu_0,\ldots,\nu_T\}$. Moreover, $R$ replicate
measurements are made, and we denote the $r$-th replicate frequency
data as $\bm{\nu}^{(r)}$.

\subsection{The \comale\  statistic}
To test if a genomic region is evolving under natural selection, we
consider a likelihood-based
approach~\cite{vitti2013detecting,nielsen2005genomic,topa2015gaussian,Terhorst2015Multi}
that (a) maximizes the likelihood of the time series data
w.r.t. selection and overdominance parameters $s,h$, for each
polymorphism in the region; and, (b) computes the log-odds ratio of
the likelihood of selection model to the likelihood of neutral
evolution/drift model, for every polymorphism in the
region. Subsequently, (c) site likelihood ratios in a genomic region
are combined to compute the \comale\ statistic, which is a composite
likelihood score for the region being under selection. In addition to
detecting selection, the \comale\ statistic can be used to rank
variants for site-identification, and it provides maximum likelihood estimates 
of selection parameters.

\paragraph{Likelihood for Neutral Model.}
To model neutral evolution, it is natural to model the change in
frequency $\nu_t$ over time via Brownian
motion~\cite{feder2014Identifying} or Gaussian
process~\cite{topa2015gaussian,Terhorst2015Multi}. Significant
deviations from this Null could be indicative of
non-neutrality. However, in our experiments, we found that the
Brownian motion approximation is inadequate for small populations, low
starting frequencies and sparse sampling (in time) that are typical in
experimental evolution (see Results, and Fig.~\ref{fig:markov}). In
fact, other ``continuous'' models such as Gaussian process for dynamic
allele frequencies, are also susceptible to this issue (see Results
and Fig.~\ref{fig:power}A-C).

Instead, by computing likelihood of data using a discrete-time
discrete-state-space Wright-Fisher Markov chain, we turn the problem
of small-population size into an advantage. Consider a neutrally
evolving diploid population with $N$ individuals. Define a
$2N\times2N$ transition matrix $P$, where $P^{(\tau)}[i,j]$ denotes
probability of change in allele frequency from $\frac{i}{2N}$ to
$\frac{j}{2N}$ in $\tau$ generations, solely due to genetic drift. $P$
is defined as follows~\cite{Ewens2012Mathematical}:
\begin{eqnarray}
  P^{(1)}[i,j] &=& \pr\left(\nu_{t+1}=\frac{j}{2N} \left|
      \nu_{t}=\frac{i}{2N}\right)={2N \choose j} \right.  \nu_{t}^j
  (1-\nu_{t})^{2N-j}, \;\;\label{eq:P1}\\
  P^{(\tau)} &=&   P^{(\tau-1)}P^{(1)} \label{eq:Pt}
\end{eqnarray}
Note that $P^{(\tau)}$ needs to be computed only once and can be reused for all 
the variants in the genome. Also, precomputing and storing $P^{(\tau)}$ is 
tractable and
numerically stable for controlled experimental evolution experiments
with $N\le5000$. 

\paragraph{Likelihood for Selection Model.}
Assume that the site is evolving under selection constraints $s\in
\Rbb$, $h\in \Rbb_+$, where $s$ and $h$ denote selection strength and 
overdominance parameters ,
respectively. By definition, the relative fitness values of genotypes
0$|$0, 0$|$1 and 1$|$1 are given by $w_{00}=1$, $w_{01}=1+hs$ and
$w_{11}=1+s$. Recall that $\nu_t$ denotes the frequency of the site at
time $\tau_t\in {\cal T}$. Then, $\nusp$, the frequency at time
$\tau_{t}+1$ (one generation ahead) can be estimated using: \beq 
\hat{\nu}_{t^+} =
\mathbb{E}[\nusp|s,h,\nu_t]&=\frac{w_{11}\nu_t^2 +
  w_{01}\nu_t(1-\nu_t)}{w_{11}\nu^2_t + 2w_{01}\nu_t(1-\nu_t) +
  w_{00}(1-\nu_t)^2}\\
&=\nu_t+\frac{s(h+(1-2h)\nu_t)\nu_t(1-\nu_t)}{1+s\nu_t(2h+(1-2h)\nu_t))}.
  \label{eq:transition}
\eeq
For finite populations, let $Q^{(\tau)}_{s,h}[i,j]$ denote the
probability of transition from $\frac{i}{2N}$ to $\frac{j}{2N}$ in
$\tau$ generations. We model $Q$ as follows
(See~\cite{Ewens2012Mathematical}, Pg.~24, Eqn.~$1.58$-$1.59$):
\begin{eqnarray}
  Q^{(1)}_{s,h}[i,j] &=& \pr\left(\nusp=\frac{j}{2N} \left\lvert
      \nu_{t}=\frac{i}{2N};s,h \right .\right)={2N \choose j}
  \hat{\nu}_{t^+}^{j} (1-\hat{\nu}_{t^+})^{2N-j}\label{eq:Q1}\\
  Q^{(\tau)}_{s,h} &=& Q^{(\tau-1)}_{s,h}Q^{(1)}_{s,h}\label{eq:Qt}
  \label{eq:mkvs}   
\end{eqnarray}
For $s=0$, Eq.~\ref{eq:Q1} and~\ref{eq:Qt} are identical to
Eq.~\ref{eq:P1} and~\ref{eq:Pt}, respectively.  The likelihood of
observing the trajectory $\bm{\nu}$ is computed using:
\begin{equation}
  \Lc_M(s,h|\bm{\nu}) = \pr(\bm{\nu};s,h)=
  \prod_{t=1}^{T} \pr(\nu_{t}|\nu_{t-1};s,h) = \prod_{t=1}^{T} 
  Q^{(\delta_t)}_{s,h}[\hat{i},\hat{j}],
%  \label{eq:mkvlik}  
\end{equation}
where, $(\hat{i},\hat{j})=( 2N\nu_{t-1}, 2N\nu_{t})$, and
$\delta_t=\tau_{t}-\tau_{t-1}$.  Combining the likelihood over
independent replicate samples $\bm{\nu}^{(r)}$, we get:
\begin{equation}
  \Lc_M(s,h|\{\bm{\nu}^{(r)}\}) = \prod_r   \Lc_M(s,h|\bm{\nu}^{(r)}).
  \label{eq:mkvlik}
\end{equation}
Let $\hat{s},\hat{h}$ denote the parameters that maximize the
likelihood. The simplest form of the Markov likelihood ratio test statistic  
for each variant is given by
\begin{eqnarray}
M=\sgn (\hat{s}). \log 
\left(\frac{\Lc_M(\hat{s},\hat{h}|\{\bm{\nu}^{(r)}\})}{\Lc_M(0,0|\{\bm{\nu}^{(r)}\})}\right).
\label{eq:mcts}
\end{eqnarray}
%where the sign $\sgn(\hat{s})$ helps focus on positive selection using a 
%one-sided test.  




\paragraph{Accounting for Heterogeneous Ascertainment Bias.}
In the discussion so far, we assumed that the exact allele frequencies
are supplied. However, in most cases, allele frequencies are 
estimated from genotype data with small sample size or pool-seq 
data~\cite{lynch2014population} 
(Suppl.~Fig.~\ref{fig:stateConditional},\ref{fig:trajectoryReal}).  Moreover,
the depth at a site varies for different replicates, and different
time samples (Suppl.~Fig.~\ref{fig:depthHetero}) and filtering low coverage 
variants can potentially discard useful information.
For instance, in analyzing \datadm~\cite{orozco2012adaptation}, by setting 
minimum read depth at a site to 
be 30,
allowing the site to be retained only if the depth for all time points, and all 
replicates exceeded $50$, the number of sites to be analyzed
would drop from from 1,733,121 to 11,232 (Suppl. Fig.~\ref{fig:depth}). To 
account for this
heterogeneity, we extend the Markov chain likelihood in
Eq.~\ref{eq:mkvlik} to a Hidden Markov Model (HMM) for pool-seq data.

Consider a variant position being sampled at time point $\tau_t\in
{\cal T}$. We denote the pool-seq data for that variant as $x_t =
\langle c_t,d_t \rangle$ where $d_t, c_t$ represent the read depth,
and the count of the derived allele, respectively, at time
$\tau_t$. The time-series data is represented by the sequence
$\bm{x}=\{x_0,x_1,x_2,\ldots,x_T\}$. We model the dynamic pool-seq
data using a an HMM with $2N+1$ states which state $i$ $(0\le i\le
2N)$ corresponds to allele frequency $i/2N$.  Also, we note that HMM
is stationary model in that transition and emission distributions do
not change over time and defining these distributions completely
specifies the HMM model. Eqs.~\ref{eq:P1},~\ref{eq:Pt} define
transition probabilities. The probability that state $i$ emits $
x=\langle d, c\rangle $ is given by
\begin{equation*}
{\bf e}_{i}(x) = {d \choose c} \left(\frac{i}{2N} \right)^c\left (1- 
\frac{i}{2N} \right)^{d-c}.
\end{equation*}
For $1\le t\le T$, let $\alpha_{t,i}$ denote the probability of
emitting $x_1,x_2,\ldots,x_t$ and reaching state $i$ at
$\tau_t$. Then, $\alpha_{t,i}$ can be computed using the
forward-procedure~\cite{durbin1998biological}:
\begin{equation}
  \alpha_{t,i} = \left( \sum_{1\le j\le 2N} 
  \alpha_{t-1,j}\;Q^{(\delta_t)}_{s,h}[j,i] \right) {\bf e}_{i}(x_t)\;\; .
  \label{eq:hmm}
\end{equation}
where $\delta_t=\tau_t-\tau_{t-1}$. The joint likelihood of the
observed data from $R$ independent observations is given by
\begin{equation}
  \Lc_{H}(s,h|\{\bm{x}^{(r)}\})=\prod_{r=1}^R\Lc_{H}(s,h|\bm{x}^{(r)}) = 
  \prod_{r=1}^R \sum_i\alpha_{T,i}^{(r)}\;\;.
  \label{eq:hmmlik}
\end{equation}
Similar to Eq.~\ref{eq:mcts}, let $\hat{s},\hat{h}$ denote the parameters that
maximize likelihood. The likelihood ratio statistic for
each variant of pool-seq data is given by
\begin{eqnarray}
H &=& \sgn (\hat{s}). \log 
\left(\frac{\Lc_H(\hat{s},\hat{h}|\{\bm{x}^{(r)}\})}{\Lc_H(0,0|\{\bm{x}^{(r)}\})}\right).
\label{eq:hmmml}
\end{eqnarray}

\paragraph{Composite Likelihood Ratio.}
In general, the favored allele can be in linkage disequilibrium with
some of its surrounding variation. The linked-loci hitchhike and share
similar dynamics with the favored allele.  Some models such as
multi-locus Gaussian process~\cite{Terhorst2015Multi} incorporate
these associations by modeling linkage and recombination
explicitly. However, these approaches are computationally
expensive. Moreover, linkage computations are difficult without
haplotype resolved data, which pool-seq does not provide. Instead, we
work with a simpler Composite Likelihood Ratio
(CLR)~\cite{nielsen2005genomic,williamson2007localizing} computation
to combine the individual scores of all variants into a composite
score.


Consider a genomic region $L$ to be a collection of segregating sites
with little or no recombination between sites and the favored
allele. This scenario holds when the starting frequency of the favored
allele is not high and the region is small. Let $M_\ell$
(respectively, $H_\ell$) denote the likelihood ratio score based on
Markov chain (respectively, HMM) for each site $\ell$ in $L$. The
classical CLR is computed by averaging scores of all the variants
withing the testing region.  However, the levels of LD between favored
allele and its surrounding variation depend on initial frequency
$\nu_0$, strength of selection $s$ and time since the onset of
selection (see Appendix~\ref{app:ld} for more details).  Hence, we
parameterize CLR to discard those polymorphisms that are in low LD
with the favored allele, from computation of CLR.  In particular, we
can choose to only include sites whose likelihood ratio score is above
a certain threshold. For percentile cut-off $\pi$, let
$L_{\pi}\subseteq L$ denote the set of sites whose likelihood ratio
scores had percentile $\pi$ or better. For all $\pi$, the modified CLR
statistic for Markov chain and HMM is computed using: 
\begin{equation}
 {\cal M}_{\pi} = \frac{1}{|L_{\pi}|}\sum_{\ell \in L_{\pi}}M_\ell,
\hspace{0.5in}
 {\cal H}_{\pi} = \frac{1}{|L_{\pi}|}\sum_{\ell \in L_{\pi}} H_\ell.
\label{eq:pihmm}
\end{equation}
In combining the sites, recall that sites that are not on the same
lineage as the favored site, will reduce in frequency and have
negative likelihood values, but are still informative about
selection. Therefore, we also define:
\begin{equation}
 {\cal M}^{+}_{\pi} = \frac{1}{|L_{\pi}|}\sum_{\ell \in L_{\pi}}\vert M_\ell\vert ,
\hspace{0.5in}
 {\cal H}^{+}_{\pi} = \frac{1}{|L_{\pi}|}\sum_{\ell \in L_{\pi}}\vert H_\ell\vert.
\label{eq:pihmmplus}
\end{equation}
We also note here that unlike $\Hc$, $\Hc^{+}_{\pi}$ can be computed
without knowledge of the ancestral allele.
\paragraph{Final Word on Notation.}
In the following, we use $M$, Eq.~\ref{eq:mcts} (respectively, $H$,
Eq.~\ref{eq:hmmml}) to denote the \comale\ scores for individual
variants. Similarly, we use $\Mc_{\pi},\Mc^{+}_{\pi}$ (respectively,
$\Hc_{\pi},\Hc^{+}_{\pi}$) to denote the composite \comale\ scores for
a genomic region. In this notation, ${\cal H}_{100}={\cal
  H}^{+}_{100}$ corresponds to the CLR based on choosing the best site
in a region, while $\Hc_0,\Hc^{+}_0$ both correspond to the CLR based
on all sites in the region. For ease of notation, we sometimes use
$\Hc$, or $\Hc^{+}$ to denote \comale\ scores in results, when the
choice of $\pi$ is apparent.

\paragraph{Site-identification.} 
We rank the individual variant scores in a region to predict the
favored site. In general, identifying the favored site in pool-seq
data is difficult~\cite{tobler2014massive}, due to extensive span of
hitchhikers in an ongoing sweep (see Appendix~\ref{app:ld} for more
detail). In our analysis of the \dmel EE data, we identify a set of
``candidate'' variants whose scores exceed a False Discovery Rate
threshold based on the distribution of \comale\ scores on negative
controls.

\paragraph{Estimating Parameters.}
\label{sec:regression}
Depending on data (read count or allele frequency) the optimal value
of the parameters can be found by 
\beqn
\hat{s},\hat{h}&=&\underset{s,h}{\arg\max} \sum_r^R \log
\left(\Lc_{\cal M}(s,h|\bm{\nu}^{(r)}\right),\;\; \mbox{ or, }\label{eq:mcmle}\\
\hat{s},\hat{h}&=&\underset{s,h}{\arg\max} \sum_r^R \log
\left(\Lc_{\cal H}(s,h|\bm{x}^{(r)}\right).\label{eq:hmmmle}
\eeqn
where likelihoods are defined in Eq.~\ref{eq:mkvlik} and
Eq.~\ref{eq:hmmlik}, respectively.  The parameters in
Eqs.~\ref{eq:mcmle},~\ref{eq:hmmmle} are optimized using grid
search. By broadcasting and vectorizing the grid search operations
across all variants, the genome scan on millions of polymorphisms can
be done in significantly smaller time than iterating a numerical
optimization routine for each variant(see Results and
Fig.~\ref{fig:runTime}). The value of the overdominance parameter can
provide extra information regarding dominance of the favored allele
and the kind of selection~\cite{gillespie2010population} (See Suppl. 
Fig.~\ref{fig:dominance},\ref{fig:dir-bal}):
\begin{center}
	\begin{tabular}{l|c}
		condition & comment\\
		\hline
		$h=0$ & recessive adaptive allele\\
		$h=0.5$ & directional selection\\
		$h=1$&	dominant adaptive allele	\\
		$h>1$ &overdominance
	\end{tabular}
\end{center}


\paragraph{Precomputing Transition Matrices.}
\comale\ requires a one-time computation of matrices
$Q^{(\tau)}_{s,h}$ for the entire range of $s,h$
values. Precomputation of $909$ transition matrices for
$s\in\{-0.5,-0.49,\ldots,0.5 \}$ and $h\in \{0,0.25,\ldots,2\}$ took
less that 15 minutes ($\approx$ 1 second per matrix) on a desktop
computer with a Core i7 CPU and 16GB of RAM.


\subsection{Extending Site Frequency Spectrum  based tests for  time series 
data}\label{sec:extending-sfs}
\label{sec:sfs-ts}
The site frequency spectrum (SFS) is a mainstay of tests of neutrality
and selection, and can be computed using pool-seq data (does not
need haplotypes). Following Fu, 1995~\cite{fu1995statistical}, any
linear combination of the site frequencies is an estimate of
$\theta$. However, under non-neutral conditions, different linear
combinations behave differently. Therefore, many popular tests of
neutrality either compute differences of two estimates of $\theta$, or
perform cross-population tests comparing the $\theta$ estimates in two
different
populations~\cite{achaz2009frequency,ronen2013learning,sabeti2007genome}. 

We asked if SFS-based tests could be adapted for time-series data. A
simple approach is to use cross-population SFS tests on the
populations at time $0$ (before onset of selection), and at time
sample $\tau_t$, for each $t$. However, these tests are not
independent. Evans \emph{et al.}~\cite{evans2007non} developed
diffusion equations for evolution of SFS in time series, but they are
difficult to solve. Instead, we derive a formula for computing $D_t$,
the dynamic of Tajima's $D$ at generation $t$. Specifically, for
initial value $D_0$, initial carrier frequency $\nu_0$ and 
selection coefficient $s$:
\begin{equation}
  D_t=D_0-\log(1-\nu_t) \frac{W_0}{\log(2N)} -\nu_t^2 \Pi_0\;,
  \label{eq:tdt}    
\end{equation}
where $W_0$ and $\Pi_0$ are Watterson's and Tajima's estimates of
$\theta$ in the initial generation (Appendix~\ref{app:td}).  See
Suppl.~Fig.~\ref{fig:sfsts} for comparison to empirical values from
simulations. Similarly, we show (Appendix~\ref{app:h}), that the
dynamics of Fay and Wu's $H$ statistic~\cite{fay2000hitchhiking} are
directly related to expected value of the of Haplotype Allele
Frequency (HAF) score~\cite{ronen2015predicting}, and can be written
as a function of $\nu_t$ as follows:
\begin{equation}
  nH_t= \theta \nu_t \left(\frac{\nu_t+1}{2} -
    \frac{1}{(1-\nu_t)n+1}\right) + \theta
  (1-\nu_t)\left(\frac{n+1}{2n}-\frac{1}{(1-\nu_t)n+1}\right)
  \label{eq:ht}
\end{equation}	
In both cases, $\nu_t$ itself can be written as a function of $s,t$
(Suppl.~Eq.~\ref{eq:inf-pop}). This allows us to compute likelihood
functions $\Lc_S(s; \{D_t\})$ or $\Lc_S(s; \{H_t\})$. Then, a
likelihood ratio, similar to Eqns.~\ref{eq:mcts},~\ref{eq:hmmml}
provides a statistic for detecting selection in each window. 

However, as $\nu_0$ and $D_0$ are often unknown in sampling from
natural populations, we do not directly use
Eqns.~\ref{eq:tdt},~\ref{eq:ht}. Instead, we heuristically aggregate
statistics throughout time to compute a time-series score
(Appendix~\ref{app:agg}).


\ignore{
\paragraph{$p$-value Computation.}
By Wilks’ theorem~\cite{williams2001weighing}, the standard likelihood
ratio statistic (Eq.~\ref{eq:mcts}) is asymptotically distributed
according to $\Xc^2$. However, Feder et
al.~\cite{feder2014Identifying} point out that the empirical
distribution provide more accurate $p$-values than $\Xc^2$ when the
number of independent samples (replicates) is small.  Here we compute
$p$-value for the test statistic using the empirical distribution of
the negative controls.}
		


\subsection{Simulations}
We performed extensive simulations using parameters that have been
used for \dmel experimental
evolution~\cite{kofler2013guide}. See also Fig.~\ref{fig:ee} for
illustration. To implement real world pool-seq experimental evolution, we 
conducted simulations as follows:
\begin{enumerate}[I.]
\item {\bf Creating initial founder line haplotypes.} Using
  \texttt{msms}~\cite{ewing2010msms}, we created neutral populations for $F$
  founding haplotypes with \emph{default} parameters \texttt{\$./msms
    <F> 1 -t <2$\mu$LNe> -r <2rNeL> <L>}, where $F=200$ is number of
  founder lines, $N_e=10^6$ is effective population size,
  $r=2*10^{-8}$ is recombination rate, $\mu=2\times 10^{-9}$ is
  mutation rate and $L=50K$ is the window size in base pairs which
  gives $\theta=2\mu N_eL=200$ and $\rho=2N_erL=2000$.
  
\item{\bf Creating initial diploid population.} To simulate
  experimental evolution of diploid organisms, initial haplotypes were
  first cloned to create $F$ diploid homozygotes. Next, each diploid
  individual was cloned $N/F$ times to yield diploid population of
  size $N$.

\item{\bf Forward Simulation.} We used forward simulations for
  evolving populations under selection. We note that all experiments
  denoted as \emph{soft sweep} below, refer to selection acting upon
  standing variation. Given initial diploid population, position of
  the site under selection, selection strength $s$, number of
  replicates $R=3$, recombination rate $r=2\times10^{-8}$ and sampling
  times $\Tc=\{0,10,20,30,40,50\}$, \texttt{simuPop} was used to
  perform forward simulation and compute allele frequencies for all of
  the $R$ replicates.  For hard sweep (respectively, soft sweep)
  simulations we randomly chosen a site with initial frequency of
  $\nu_0=0.005$ (respectively, $\nu_0=0.1$) to be the favored allele.
\item{\bf Sequencing Simulation.} Give allele frequency trajectories
  we sampled depth of each site identically and independently from
  Poisson($\lambda$), where $\lambda \in \{30,100,\infty\}$ is the
  coverage for the experiment. Once depth $d$ is drawn for the site
  with frequency $\nu$, the number of reads $c$ carrying the derived
  allele are sampled according to Binomial$(d,\nu)$. For experiments
  with finite depth the tuple $\langle c,d\rangle$ is the input data
  for each site. Infinite depth experiments refer to the case, where
  the true allele frequency is provided and Markov and HMM likelihood
  computations give identical results.
\end{enumerate}
We also conducted simulations to evaluate performance for cases
involving evolution and re-sequencing of natural populations. Sampling
from natural populations differs from (controlled) experimental
evolution in some important ways. First, the time of onset of
selection may not be known. Second, as the start of sampling can be
any generation during selective sweep, mutations that arose after the
onset of selection appear in data can have a nontrivial effect on SFS,
specifically if sampling is started long after onset of selection. On
the other hand, the power of detection of selection is highest near
fixation, and larger population sizes help provide more robust
estimates of deviation from neutrality. To simulate these scenarios,
\texttt{msms} was used to forward-simulate a population with
$N_e=10^4$, $\nu_0=10^{-4}$, and to record SFS of a 50Kbp region
(Fig.~\ref{fig:ee}A). The remaining parameters were identical to
controlled experimental evolution simulations.

\paragraph{False Discovery Rate (FDR).} We applied \comale\ to the
\datadm~\cite{orozco2012adaptation}, by computing the $\Hc^{+}$
statistic for sliding windows of 30Kbp with steps of 10Kbp over the
entire genome. Due to the great variation in the density of polymorphic
sites, we observed a large variation in scores. Moreover, high LD
between proximal sites resulted in many neighboring windows with
similar scores. Therefore, we computed a local false discovery rate
for any candidate window by choosing an encompassing genomic region of
$2$Mbp to preserve the genomic background. For each candidate window,
we sampled a subset of variants from the encompassing region $10,000$
times, and computed the $\Hc^+$ score. The candidate window was
selected if its score was among the top $1\%$ of scores in the
permuted tests.
