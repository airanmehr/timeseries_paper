\section{Methods}
In this section we formally present describe RNN model and a Naive method which takes $\Oc(1)$ computations as baseline performance.

\paragraph{Notation.}
\sout{In this paper, we denote Tensor with bold-faced capital letter,
  matrices with capital letter, vectors with bold-faced small letter,
  and scalars with small letters.}

The allele frequencies of a population are given by the tensor $\bfX
\in [0,1]^{R \times L \times T}$, where $R,L,T$ represent the number
of experimental replicates, segregating sites, and samples in time,
respectively. Correspondingly, let $X_r=[\bfx_1 ,\ldots, \bfx_L]\in
[0,1]^ {T \times L}$ denote the data matrix for replicate $r$. Note
that the sampling times may not be not equally spaced, but are given
by the set $\Tc=\{\tau_1,\tau_2,\cdot, \tau_T \}$. For each locus
$\ell$, $\bfx_{\ell} =[x_{\tau_1},\ldots,x_{\tau_T}]$ represent the
observed allele frequencies at different time points (measured in
generations). \sout{In the discussion below, we use $x_t\in [0,1]$ to
  denote the allele frequency at time $\tau_t$ for a specific
  replicate and locus. As an example, if $\Tc=\{2,18,25\}$, then
  $x_1,x_2,x_3$ denote the allele frequencies at the $2$nd, $18$th and
  $25$th generation, respectively.}

\VB{Some confusion in notation here, because $x_t$ was used to denote
  allele frequency at time $\tau_t$, but in the discussion below, it
  referred to allele frequency at time $t$. We should just use
  $x_t$ to denote allele frequency at time $t$}.

\paragraph{Allele frequencies under selection.}
Consider a bi-allelic single-locus (Wright-Fisher like) model with no
mutations \cite{book-mathpopgen}, discrete generations, random mating
etc. With finite population size, the allele frequencies from
generation to generation are described 
\beq\label{eq:trans0}
x_{t+1} = f(x_t;s,h)  + \epsilon_t\footnote{In infinite population
  size we have $x_{t+1} = f(x_t;s,h)$.} 
\eeq

where $h$ is the overdominance, $\epsilon_t$ is random noise due to
genetic drift at generation $t$ and $f: [0,1] \mapsto [0,1]$ is the
transition function:

\beq
f(x)=\frac{(1+s)x^2 + (1+hs)x(1-x)}{(1+s)x^2 + 2(1+hs)x(1-x) + (1-x)^2}=x+\frac{s(h+(1-2h)x)x(1-x)}{1+sx(2h+(1-2h)x))}
\eeq

We simplify notation by setting $h=0.5$, so that
\beq
f(x)=x+\frac{sx(1-x)}{2+2sx}
\eeq

Our goal is to estimate $s$ given the observed frequencies. Note that
the dependence on $s$ is non-linear. Specifically, at time $t+\delta
t$, define the auxiliary continuous variable $y_t$ as follows:
$y_0=x_0$, and:
\begin{eqnarray}
  y_{t+\delta t} &=& y_t+\frac{y_t(1-y_t)s\delta t}{2+2s\delta t y_t}\\
  \frac{dy_t}{dt} &=&\lim_{\delta t\rightarrow 0}\frac{y_{t+\delta t} -y_t}{\delta t}\\
   &=&\lim_{\delta t\rightarrow 0}\frac{sy_t(1-y_t)}{2+2y_ts\delta t}\\
   &=& \frac{s}{2}y_t(1-y_t) 
  \label{eq:ode}
\end{eqnarray}
The ODE can be readily solved for $y_t$ as
\beq
y_t =\frac{1}{1+\frac{1-x_0}{x_0}e^{-st/2}} = \sigma(c-st/2) 
\label{eq:inf-pop} 
\eeq

where $c=\ln\left(\frac{1-x_0}{x_0}\right)$ is constant
\cite{multilocus-hitchhike} and $\sigma(.)$ is a sigmoid
function. Finally, we note that for $y_t=\mathbb{E}[x_t]$ for all $t$,
and therefore, can be used to estimate $x_t$. Our goal is to compute
$s$ so that the observed and expected scores best match.

\paragraph{Naive two-point optimization.} 
For each $t\in \Tc$, we use \eqref{eq:inf-pop} to provide a naive
estimate $s(t)$ for the selection coefficient as

\beq 
s(t)=\frac{2}{t} \log \left( \frac{x_t(1-x_0)}{x_0 (1-x_t)} \right) 
\eeq

To reduce the variance we can average all the naive estimates: 

\beq
s_{N}=\frac{1}{|\Tc|}\sum_{t\in \Tc}\frac{2}{t} \log \left( \frac{x_t
    (1-x_0)}{x_0 (1-x_t)} \right) 
\label{eq:naive}
\eeq


\paragraph{Regularized Nonlinear Least Squares.}
For a given value of $s$, we have the estimated allele frequencies at
a locus $\ell$ given by
\[
{\bf y_{\ell}} = [y_{\tau_1},\ldots,y_{\tau_T}]
\]
where $y_t=\sigma(c-st/2)$. The observed allele frequencies are
given by
\[
{\bf x_{\ell}} = [x_{\tau_1},\ldots,x_{\tau_T}]
\]

Our goal is to compute:

\beq \label{eq:nlls0}
s^*=\underset{s}{\arg \min} \parallel {\bf y_{\ell}(s) -  x_{\ell}} \parallel_2
\eeq

Setting $y_t(s)=\sigma(c-st/2)$, and $L_2$ regularization, we compute

\beq
s^*=\underset{s}{\arg \min} \sum_{t\in \Tc} \frac{1}{2} \vert \sigma(c-st/2)- x_{t} \vert^2 + \frac{\lambda }{2}s^2
\eeq

which is a standard 1-d nonlinear regularized nonlinear least squares
(RNLLS) optimization problem. To solve it we need to use iterative
optimization algorithms which require computing
gradient\footnote{$\sigma'(s)=\sigma(s)(1-\sigma(s))$} w.r.t. $s$

\beq \label{eq:grad}
g= \sum_{t\in \Tc} \frac{t}{2} ( \sigma(st/2)- x_{t} ) \sigma(st/2) (1-\sigma(st/2)) + \lambda s
\eeq

By taking into account of all replicates, i.e., computing
\ref{eq:grad} for every replicate\footnote{Since all the replicates
  are iid, we can just sum them up in the objective function.}, the
gradient descent update is

\beq
s\leftarrow s - \eta \sum_{r=1}^R g_r
\eeq

where $\eta$ is learning rate and $g_r$ is the gradient of the
objective for $r^{th}$ replicate data at examining loci. Also, since
this problem is not convex, we use Nestrov's Accelerated Gradient
(NAG) descent algorithm \cite{â€¢} which which has shown to be
successful on many nonconvex problems. Also it should be noted that
each iteration of the optimization takes $\Oc(TR)$ computation which
make the algorithm tractable.

\subsection{Gaussian Process}
The Gaussian Process optimizes
\beq
\underset{\theta}{ \arg \max} \ \ \Lc(\bfX | \theta)
\eeq
where $\Lc$ is Gaussian distribution log-likelihood, i.e. negative weighted least-squares loss. Mean and covariance functions of the GP at any $t,l$ are functionally dependent to parameter of interest $\theta$, and computed using transition function of the WF process \cite{EandR-GP}.

\subsection{Naive Method}
