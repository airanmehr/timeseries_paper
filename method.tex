\section{Methods}
In this section we formally present describe RNN model and a Naive method which takes $\Oc(1)$ computations as baseline performance.

\paragraph{Notation.}
In this paper, we aim to model random process $\{x_t\}$ for every loci, where  $x_t$ is the population allele frequency at generation $t$. The model have access to finite number of observations in time for each locus, where the sampling times are given by the set $\Tc=\{\tau_1,\tau_2,\cdots, \tau_T \}$ such that $\tau_1<\tau_2,\cdots<\tau_T$. We also assume that measurements are taken up for $R$ experimental replicates for $L$ loci. Thus, The allele frequencies of a population are given by the tensor $\bfX
\in [0,1]^{R \times L \times T}$, where $X_{r,\ell,t}$ stores the
observed of allele frequency at replicate $r$, locus $\ell$ and generation $\tau_t$.

\paragraph{Allele frequencies under selection.}
Consider a bi-allelic single-locus (Wright-Fisher like) model with no
mutations \cite{book-mathpopgen}, discrete generations, random mating
etc. With finite population size, the allele frequencies from
generation to generation are described 
\beq\label{eq:trans0}
x_{t+1} = f(x_t;s,h)  + \epsilon_t\footnote{In infinite population
  size we have $x_{t+1} = f(x_t;s,h)$.} 
\eeq

where $h$ is the overdominance, $\epsilon_t \sim \Nc(0,1)$ is random noise due to
genetic drift at generation $t$ and $f: [0,1] \mapsto [0,1]$ is the
transition function:

\beq
f(x)=\frac{(1+s)x^2 + (1+hs)x(1-x)}{(1+s)x^2 + 2(1+hs)x(1-x) + (1-x)^2}=x+\frac{s(h+(1-2h)x)x(1-x)}{1+sx(2h+(1-2h)x))}
\eeq

We simplify notation by setting $h=0.5$, so that
\beq
f(x)=x+\frac{sx(1-x)}{2+2sx}
\eeq

Our goal is to estimate $s$ given the observed frequencies. Note that
the dependence on $s$ is non-linear. Specifically, at time $t+\delta
t$, define the auxiliary continuous variable $y_t$ as follows:
$y_0=x_0$, and:
\begin{eqnarray}
  y_{t+\delta t} &=& y_t+\frac{y_t(1-y_t)s\delta t}{2+2s\delta t y_t}\\
  \frac{dy_t}{dt} &=&\lim_{\delta t\rightarrow 0}\frac{y_{t+\delta t} -y_t}{\delta t}\\
   &=&\lim_{\delta t\rightarrow 0}\frac{sy_t(1-y_t)}{2+2y_ts\delta t}\\
   &=& \frac{s}{2}y_t(1-y_t) 
  \label{eq:ode}
\end{eqnarray}
The ODE can be readily solved for $y_t$ as
\beq
y_t =\frac{1}{1+\frac{1-x_0}{x_0}e^{-st/2}} = \sigma(st/2-c) 
\label{eq:inf-pop} 
\eeq

where $c=\log\left(\frac{1-x_0}{x_0}\right)$ is constant
\cite{multilocus-hitchhike} and $\sigma(.)$ is a sigmoid
function. Finally, we note that for $x_t\sim\Nc(y_t,1)$ for all $t$, 
and therefore observations $x_t$ can be used to fine Maximum Likelihood Estimate of $y_t$ and since sigmoid is a one-to-one function, we can find MLE for 
$s$.

\paragraph{Naive two-point optimization.} 
For each $t\in \Tc$, we use \eqref{eq:inf-pop} to provide a naive
estimate $s(t)$ for the selection coefficient as

\beq 
s(t)=\frac{2}{t} \log \left( \frac{x_t(1-x_0)}{x_0 (1-x_t)} \right) 
\eeq

To reduce the variance we can average all the naive estimates: 

\beq
s_{N}=\frac{1}{|\Tc|}\sum_{t\in \Tc}\frac{2}{t} \log \left( \frac{x_t
    (1-x_0)}{x_0 (1-x_t)} \right) 
\label{eq:naive}
\eeq


\paragraph{Maximum Likelihood Estimate.}
For a given value of $s$, we have the estimated allele frequencies at
a locus $\ell$  and replicate $r$ given by
\[
{\bfy_{\ell}(s)} = [y_{\tau_1},\ldots,y_{\tau_T}]
\]
where\footnote{Note that since $x_0$ is the same for all the replicates, so is $c$. Therefore $\bfy(s)$ is equal for all $r$. } $y_{t}=\sigma(st/2-c)$. The observed allele frequencies are
given by
\[
{\bfx_{r,\ell}} = [x_{r,\ell,\tau_1},\ldots,x_{r,\ell,\tau_T}]
\]

By assuming that the random noise due to genetic drift at each generation is independent of other generations\footnote{I think it $\Sigma(\epsilon_{\tau_1},\epsilon_{\tau_2})$ should depend on $N_e$ and $|\tau_1-\tau_2|$. Song, uses $\Sigma(\epsilon_{\tau_1},\epsilon_{\tau_2}) \approx |\tau_1-\tau_2|(1-\frac{|\tau_1-\tau_2|}{4N})$}, i.e. $\Sigma_\epsilon = I$, and the fact that replicates are iid, the Gaussian likelihood function is
\beq
\Lc_G(s|\bfx_{1,\ell}, \dots, \bfx_{R,\ell}) = \prod_{r=1}^R \Pr(\bfx_{r,\ell}| \mu=\bfy_\ell(s), \Sigma= I)
\eeq
where $\bfx_{r,\ell} \sim \Nc(\bfy(s),I)$. By taking log and removing constant values, the problem of finding MLE for $s$ amounts to optimizing the negative-log-likelihood function with respect to $s$:
\beq \label{eq:nlls0}
s^*=\underset{s}{\arg \min} \frac{1}{2} \sum_{r=1}^R \parallel {\bfy_{\ell}(s) -  \bfx_{r,\ell}} \parallel_2^2
\eeq
which is an instance of non-linear least squares optimization.
Setting $y_t(s)=\sigma(st/2-c)$, and $L_2$ regularization\footnote{In addition to statistical and optimization advantages, $L_2$ regularization here posits an assumption that the model is agnostic to strong selections. This completely make sense, since large values of $s$ rapidly drives $x$ to fixation.}, we compute

\beq
s^*=\underset{s}{\arg \min} \frac{1}{2}  \sum_{r=1}^R\sum_{t\in \Tc} \left( \sigma(st/2-c)- x_{r,\ell,t} \right)^2 + \frac{\lambda }{2}s^2
\eeq

which is a standard 1-d nonlinear regularized nonlinear least squares
(RNLLS) optimization problem and $\lambda$ is the regularization hyperparameter to trade-off between minimizing error and regularizing $s$. To solve it we need to use iterative
optimization algorithms which require computing
gradient\footnote{$\sigma'(s)=\sigma(s)(1-\sigma(s))$} w.r.t. $s$

\beq \label{eq:grad}
g_\ell= \frac{t}{2}  \sum_{r=1}^R \sum_{t\in \Tc}  ( \sigma(st/2-c)- x_{r,\ell,t} ) \sigma(st/2-c) (1-\sigma(st/2)) + \lambda s
\eeq

the gradient descent update is

\beq
s\leftarrow s - \eta  g_\ell
\eeq

where $\eta$ is learning rate. Also, since
this problem is not convex, we use Nestrov's Accelerated Gradient
(NAG) descent algorithm \cite{â€¢} which which has shown to be
successful on many nonconvex problems. Also it should be noted that
each iteration of the optimization takes $\Oc(TR)$ computation which
make the algorithm tractable.

\paragraph{Gaussian Process}
The single locus Gaussian Process optimizes
\beq
\Lc_{GP}(s|\bfx_{1,\ell}, \dots, \bfx_{R,\ell})
\eeq
where $\Lc_{GP}$ is the likelihood function  of Gaussian process. Mean and covariance functions of the GP at any $t,\ell$ are functionally dependent to parameter of interest $\theta$, and computed using transition function of the WF process \cite{EandR-GP}.
