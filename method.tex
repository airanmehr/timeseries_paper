\section{Methods}
\sout{In this section we formally present describe RNN model and a
  Naive method which takes $\Oc(1)$ computations as baseline
  performance.}  

We start with a description of `single locus multiple-time point'
models in which each locus is analyzed independently for evidence of a
selection constraint. In the subsequent section, we extend the
methodology to `linked-loci, multiple time' models where we consider
all linked loci within a small recombination-free window.

\subsection*{Single Locus Models}
We model random process $\{x_t\}$ for every loci, where $x_t$ is the
population allele frequency at generation $t$. The model have access
to finite number of observations in time for each locus, where the
sampling times are given by the set $\Tc=\{\tau_1,\tau_2,\cdots,
\tau_T \}$ such that $\tau_1<\tau_2,\cdots<\tau_T$. We also assume
that measurements are taken up for $R$ experimental replicates for $L$
loci. Thus, The allele frequencies of a population are given by the
tensor $\bfX \in [0,1]^{R \times L \times T}$, where $X_{r,\ell,t}$
stores the observed of allele frequency at replicate $r$, locus $\ell$
and generation $\tau_t$.

\paragraph{Allele frequencies under selection.}
Consider a bi-allelic single-locus (Wright-Fisher like) model with no
mutations \cite{book-mathpopgen}, discrete generations, random mating
etc. With finite population size, the allele frequencies from
generation to generation are described 
\beq\label{eq:trans0}
x_{t+1} = f(x_t;s,h)  + \epsilon_t\footnote{In infinite population
  size we have $x_{t+1} = f(x_t;s,h)$.} 
\eeq

where $h$ is the overdominance, $\epsilon_t \sim \Nc(0,1)$ is random noise due to
genetic drift at generation $t$ and $f: [0,1] \mapsto [0,1]$ is the
transition function:

\beq
f(x)=\frac{(1+s)x^2 + (1+hs)x(1-x)}{(1+s)x^2 + 2(1+hs)x(1-x) + (1-x)^2}=x+\frac{s(h+(1-2h)x)x(1-x)}{1+sx(2h+(1-2h)x))}
\eeq

We simplify notation by setting $h=0.5$, so that
\beq
f(x)=x+\frac{sx(1-x)}{2+2sx}
\eeq

Our goal is to estimate $s$ given the observed frequencies. Note that
the dependence on $s$ is non-linear. Specifically, at time $t+\delta
t$, define the auxiliary continuous variable $y_t$ as follows:
$y_0=x_0$, and:
\begin{eqnarray}
  y_{t+\delta t} &=& y_t+\frac{y_t(1-y_t)s\delta t}{2+2s\delta t y_t}\\
  \frac{dy_t}{dt} &=&\lim_{\delta t\rightarrow 0}\frac{y_{t+\delta t} -y_t}{\delta t}\\
   &=&\lim_{\delta t\rightarrow 0}\frac{sy_t(1-y_t)}{2+2y_ts\delta t}\\
   &=& \frac{s}{2}y_t(1-y_t) 
  \label{eq:ode}
\end{eqnarray}
The ODE can be readily solved for $y_t$ as
\beq
y_t =\frac{1}{1+\frac{1-x_0}{x_0}e^{-st/2}} = \sigma(st/2-c) 
\label{eq:inf-pop} 
\eeq

where $c=\log\left(\frac{1-x_0}{x_0}\right)$ is constant
\cite{multilocus-hitchhike} and $\sigma(.)$ is a sigmoid
function. Finally, we note that for $x_t\sim\Nc(y_t,1)$ for all $t$, 
and therefore observations $x_t$ can be used to fine Maximum Likelihood Estimate of $y_t$ and since sigmoid is a one-to-one function, we can find MLE for 
$s$.

\paragraph{Naive two-point optimization.} 
For each $t\in \Tc$, we use \eqref{eq:inf-pop} to provide a naive
estimate $s(t)$ for the selection coefficient as

\beq 
s(t)=\frac{2}{t} \log \left( \frac{x_t(1-x_0)}{x_0 (1-x_t)} \right) 
\eeq

To reduce the variance we can average all the naive estimates: 

\beq
s_{N}=\frac{1}{|\Tc|}\sum_{t\in \Tc}\frac{2}{t} \log \left( \frac{x_t
    (1-x_0)}{x_0 (1-x_t)} \right) 
\label{eq:naive}
\eeq


\paragraph{Maximum Likelihood Estimate.}
For a given value of $s$, we have the estimated allele frequencies at
a locus $\ell$  and replicate $r$ given by
\[
{\bfy_{\ell}(s)} = [y_{\tau_1},\ldots,y_{\tau_T}]
\]
where\footnote{Note that since $x_0$ is the same for all the replicates, so is $c$. Therefore $\bfy(s)$ is equal for all $r$. } $y_{t}=\sigma(st/2-c)$. The observed allele frequencies are
given by
\[
{\bfx_{r,\ell}} = [x_{r,\ell,\tau_1},\ldots,x_{r,\ell,\tau_T}]
\]

By assuming that the random noise due to genetic drift at each generation is independent of other generations\footnote{I think it $\Sigma(\epsilon_{\tau_1},\epsilon_{\tau_2})$ should depend on $N_e$ and $|\tau_1-\tau_2|$. Song, uses $\Sigma(\epsilon_{\tau_1},\epsilon_{\tau_2}) \approx |\tau_1-\tau_2|(1-\frac{|\tau_1-\tau_2|}{4N})$}, i.e. $\Sigma_\epsilon = I$, and the fact that replicates are iid, the Gaussian likelihood function is
\beq
\Lc_G(s|\bfx_{1,\ell}, \dots, \bfx_{R,\ell}) = \prod_{r=1}^R \Pr(\bfx_{r,\ell}| \mu=\bfy_\ell(s), \Sigma= I)
\eeq
where $\bfx_{r,\ell} \sim \Nc(\bfy(s),I)$. By taking log and removing constant values, the problem of finding MLE for $s$ amounts to optimizing the negative-log-likelihood function with respect to $s$:
\beq \label{eq:nlls0}
s^*=\underset{s}{\arg \min} \frac{1}{2} \sum_{r=1}^R \parallel {\bfy_{\ell}(s) -  \bfx_{r,\ell}} \parallel_2^2
\eeq
which is an instance of non-linear least squares optimization.
Setting $y_t(s)=\sigma(st/2-c)$, and $L_2$ regularization\footnote{In addition to statistical and optimization advantages, $L_2$ regularization here posits an assumption that the model is agnostic to strong selections. This completely make sense, since large values of $s$ rapidly drives $x$ to fixation.}, we compute

\beq
s^*=\underset{s}{\arg \min} \frac{1}{2}  \sum_{r=1}^R\sum_{t\in \Tc} \left( \sigma(st/2-c)- x_{r,\ell,t} \right)^2 + \frac{\lambda }{2}s^2
\eeq

which is a standard 1-d nonlinear regularized nonlinear least squares
(RNLLS) optimization problem and $\lambda$ is the regularization hyperparameter to trade-off between minimizing error and regularizing $s$. To solve it we need to use iterative
optimization algorithms which require computing
gradient\footnote{$\sigma'(s)=\sigma(s)(1-\sigma(s))$} w.r.t. $s$

\beq \label{eq:grad}
g_\ell= \frac{t}{2}  \sum_{r=1}^R \sum_{t\in \Tc}  ( \sigma(st/2-c)- x_{r,\ell,t} ) \sigma(st/2-c) (1-\sigma(st/2)) + \lambda s
\eeq

the gradient descent update is

\beq
s\leftarrow s - \eta  g_\ell
\eeq

where $\eta$ is learning rate. Also, since
this problem is not convex, we use Nestrov's Accelerated Gradient
(NAG) descent algorithm~\cite{XXX} which has shown to be successful on
many nonconvex problems. Also it should be noted that each iteration
of the optimization takes $\Oc(TR)$ computation which make the
algorithm tractable.


\paragraph{Gaussian Process}
The single locus Gaussian Process \cite{} optimizes
\beq
\Lc_{GP}(s|\bfx_{1,\ell}, \dots, \bfx_{R,\ell})
\eeq
where $\Lc_{GP}$ is the likelihood function  of Gaussian process. Mean and covariance functions of the GP at any $t,\ell$ are functionally dependent to parameter of interest $\theta$, and computed using transition function of the WF process \cite{EandR-GP}.

\subsection*{Linked-loci model}
Similar to single loci, we denote allele frequencies at locus $\ell$ by
\[
{\bf x_{\ell}} = [x_{\ell,\tau_1},\ldots,x_{\ell,\tau_T}]
\]
Note that we added the locus to the subscripts to help with the
exposition of multiple loci. Consider a collection of $m$ linked
loci. Represent the derived allele frequencies at time $t\in \Tc$
using:
\[
{\bf x_t} = [x_{1,t},x_{2,t}\ldots,x_{m,t}]^T 
\]
At any time $t$, suppose we have a sample of $n$ individual haplotypes
that were described by the $n\times m$ matrix $M_t$. Each row of $M_t$
represents a haplotype from the sample, and is denoted by vector
$\mathbf{h} \in \{0,1\}^m$. Let ${\bf e} = [1]^{(n)}$ denote the
$n$-dimensional unit vector. In Pooled sequencing, $M_t$ is hidden,
but by definition,
\[
 \mathbf{x_t} = \frac{1}{n} M_t^T\mathbf{e}
\]
We recently devised the $1\dHAF$ statistic~\cite{Ronen2015} for a
haplotype, and proved some bounds on its expected value during neutral
evolution, and under selection constraints. In the current notation,
the $1\dHAF$ score of haplotype $h$ is given by
\begin{equation}
1\dHAF(\mathbf{h})=n\mathbf{x_t} \cdot \mathbf{h}
\;.
\label{eq:1-HAF_SNPmatrix}
\end{equation}
The average $1\dHAF$ score at time $t$ can be estimated as:
\begin{equation} 
\mathbb{E}[1\dHAF(t)]=\frac{1}{n}\sum_h 1\dHAF(\mathbf{h}) = n\parallel \mathbf{x_t}\parallel^2
\end{equation} 
However, in an ongoing selective sweep, the expected $1\dHAF$ score is
a function of the selection coefficient $s$, and the frequency $\nu$
of the carriers of the favored mutation. From~\cite{Ronen2015}, we
find that

\begin{align}
  \mathbb{E}[1\dHAF(t)]\approx\;\; & \theta n \nu_t \left(\frac{\nu_t+1}{2} - \frac{1}{(1-\nu_t)n+1}\right) +\\
   & \theta n(1-\nu_t)\left(\frac{1}{2}+\frac{1}{2n}-\frac{1}{(1-\nu_t)n+1}\right) \\.
  \label{eq:hafscorepooled}
\end{align}
Rearranging terms, we can estimate $\nu_t$ using
\begin{equation}
\arg\min_{\nu_t} \left  \vert \theta \nu_t \left(\frac{\nu_t+1}{2} - \frac{1}{(1-\nu_t)n+1}\right) + \theta (1-\nu_t)\left(\frac{1}{2}+\frac{1}{2n}-\frac{1}{(1-\nu_t)n+1}\right) -\parallel \mathbf{x}\parallel^2 \right \vert
  \label{eq:pooledfrequency}
\end{equation}

Additionally,
\begin{equation}
  \frac{d\nu_t}{dt} = \frac{s}{2}\nu_t(1-\nu_t)   
\end{equation}
Therefore, we can estimate $s$ using
\begin{equation}
  \nu_t =\frac{1}{1+\frac{1-\nu_0}{\nu_0}e^{-st/2}} = \sigma(st/2-d) 
  \label{eq:labeledpooled_s}
\end{equation}
where $d=\log\left(\frac{1-\nu_0}{\nu_0}\right)$.

\Arya{I'm changing Vineet's notation. I explain it using my notation, we'll integrate it if see it's correct.}

Let $M_{r,\tau_t}\in \{0,1\}^{m\times n }$ be SNP matrix ($m$ individual, $n$ mutations) at generation $\tau_t$ and replicate $r$. We have $x_{r,t,\ell}=\bfone^T\bfm_{r,t,\ell}$ where $\bfm_{r,t,\ell}$ is $\ell^{th}$ column of $M_{r,\tau_t}$. \eqref{eq:1-HAF_SNPmatrix} defined $h_{r,t}=\bfone^T M_{r,\tau_t} M_{r,\tau_t}^T \bfone= \|M_{r,\tau_t}\|^2_F$

\beqq
\bfh_{r} =[h_{r,\tau_1},\ldots,h_{r,\tau_T}] \\
\bfnu(s) = [\nu_{\tau_1},\ldots,\nu_{\tau_T}]
\eeqq

\beq \label{eq:nlls1}
s^*=\underset{s}{\arg \min} \frac{1}{2} \sum_{r=1}^R \parallel {\bfnu(s) -  \bfh_{r}} \parallel_2^2
\eeq

In other words we have
\beq
h_{r,t}=<\bfx_{r,t},\bfh_{r,t}> =\bfx_{r,t}^T\bfh_{r,t}
\eeq
which $<.,.>$ is standard (Cartesian) dot product, which does not incorporate correlations between axes (bases in $n$-dimensional space ). We can incorporate such a correlation, i.e. Linkage Disequilibrium, by performing weighted dot product
\beq
h_{r,t}=<\bfx_{r,t},\bfh_{r,t}>_{C_{r,t}} = \bfx_{r,t}^TC_{r,t}\bfh_{r,t}
\eeq
where $C_{r,t}\in [-1,1]^{n \times n}$ is the LD matrix between all the sites.

\Arya{we'll talk about it in the Monday's meeting, I gotta prepare some results :)}
