\section{Materials and Methods}
\label{sec:method}
To identify the genes and variants that are responding to the selection 
pressure, we
consider the following statistical procedure:
\begin{enumerate}[(i)]
	\item  {\bf Estimating population size.} The procedure starts by 
	finding the
	maximum likelihood estimate of population size, 
	$\hN$, over the whole genome.
	\item {\bf Estimating selection parameters.} Given $\hN$, 
	maximizing the 
	likelihood of the time series data w.r.t. 
	selection and overdominance parameters $s,h$, for each	
	polymorphism.
	\item {\bf Computing likelihood statistics.} For each variant, it 
	calculates the log-odds 
	ratio of the likelihood of selection model to the likelihood of neutral
	evolution/drift model. Likelihood ratios in a genomic region
	are combined to compute the \comale\ statistic. 
	\item {\bf Hypothesis testing.} The null distribution of the \comale\ (or 
	likelihood ratio) 
	statistics are computed on 
	a set of whole-genome (single locus, respectively) drift simulations with 
	population size of $\hN$, and variant starting frequency and coverage of 
	the experimental data. Given the null distribution of statistics, 
	$p$-values and corresponding False Discovery Rate (FDR) are calculated.
	 The overlapping genes with the 
	regions (or variants) that 
	satisfy FDR criterion, 
	will be reported for functional analysis or imported to the Gene Set 
	Enrichment Analysis (GSEA).
\end{enumerate}
In the subsequent of this section, we outline different steps of the 
statistical procedure.
\subsection{Estimating Population Size}
Estimating population sizes from temporal neutral evolution data has 
been previously studied~\cite{williamson1999using,anderson2000monte, 
bollback2008estimation, Terhorst2015Multi,jonas2016estimating}. Existing 
methods are well 
designed 
for when 
the allele 
frequencies are computed from a finite sample, that is the 
ascertainment bias 
is uniform over the genome. However, in the case of pool-seq data in 
addition 
to uniform ascertainment bias, 
each variant is sampled at different rate, due to finite sequencing 
coverage. In addition, bulk of existing 
models~\cite{bollback2008estimation,feder2014Identifying,topa2015gaussian,Terhorst2015Multi}
are designed for large populations, and model frequency as a 
continues quantity. However, we show that smooth approximations is 
inadequate for small populations, low
starting frequencies and sparse sampling (in time) that are typical in
experimental evolution (see Results, \ref{fig:power}A-C, and 
\ref{fig:markov}). To this end, we model Wright-Fisher Markov process for 
generating pool-seq data~(\ref{proc:arya}) via a \emph{discrete} 
HMM,~\ref{fig:1}-B. 
In order to find an estimate of population size, we first need to find 
likelihood of the population size given neutral pool-seq data.


\paragraph{Likelihood for Neutral Model.}
Consider a neutrally evolving diploid population with fixed size of  
$N$ 
individuals where 
$\nu_t$ denotes allele frequency of the derived allele at generation 
$t$.
Experimental evolution for $R$ replicates is conducted so that at 
generations 
 $\Tc= \{\tau_i: 0\le \tau_0<\tau_1,\ldots< \tau_T\}$, $n$ individuals 
 are chosen for sequencing. 
 

 At the highest level, the consecutive allele frequencies of the population in 
 a fixed-size Wright-Fisher model evolves by Binomial sampling
 \beq
 \nu_0 \sim \pi, \hspace{1in} 2N\nu_t|\nu_{t-1} \sim \bino(2N,\nu_{t-1})
 \eeq
where $\pi$ is the global (marginal) distribution of allele frequencies in the 
base 
population. In general, $\pi$ depends on demographic history of the founder 
lines. Here we simply assume is $\pi$ is the site frequency spectrum of fixed 
sized neutral population~\ref{fig:sfs}.

To compute distributions after $\tau$ transitions, it is enough to specify the
$2N\times2N$ transition matrix $P^{(\tau)}$, where $P^{(\tau)}[i,j]$ denotes
probability of change in allele frequency from ${i}/{2N}$ to
${j}/{2N}$ in $\tau$ generations:
\begin{eqnarray}
  P^{(1)}[i,j] &=& \pr\left(\nu_{t+1}=\frac{j}{2N} \left|
      \nu_{t}=\frac{i}{2N}\right)={2N \choose j} \right.  \nu_{t}^j
  (1-\nu_{t})^{2N-j}, \;\;\label{eq:P1}\\
  P^{(\tau)} &=&   P^{(\tau-1)}P^{(1)} \label{eq:Pt}
\end{eqnarray}

As at each generation $n$ out of $N$  individuals are randomly selected 
for sequencing. The {sampled allele frequencies}, $\{y_{t}\}_{t\in\Tc}$, 
are also Binomially distributed
\beq
2ny_{t} \sim \text{Binomial}(2n,\nu_t)
\eeq
We introduce the $2N\times2n$ sampling matrix $Y$, where $Y[i,j]$ stores the 
probability that the sample allele frequency is ${i}/{2n}$ given that the 
true 
allele frequency is ${i}/{2N}$.

\ignore{In the discussion so far, we assumed that the exact allele 
frequencies
are supplied. However, in most cases, allele frequencies are estimated
from genotype data with small sample size or pool-seq
data~\cite{lynch2014population}
(\ref{fig:stateConditional} and 
\ref{fig:trajectoryReal}).
In
pool-seq datasets, the sequencing depth at a site varies for different
replicates, and different time samples
(\ref{fig:depthHetero}). Therefore, filtering low 
coverage
variants can potentially discard useful information.  For instance, in
analyzing \datadm~\cite{orozco2012adaptation,franssen2015patterns}, by 
setting 
minimum read
depth at a site to be 50, allowing the site to be retained only if the
depth for all time points, and all replicates exceeded $50$, the
number of sites to be analyzed would drop from 1,733,121 to
11,232 (\ref{fig:depth}). To account for this
heterogeneity, we extend the Markov chain likelihood in
Eq.~\ref{eq:mkvlik} to a Hidden Markov Model (HMM) for pool-seq data.}

We denote the pool-seq data for that variant as $\{x_t =
\langle c_t,d_t \rangle\}_{t\in\Tc}$ where $d_t, c_t$ represent the read depth,
and the read count of the derived allele, respectively, at time
$\tau_t$. Let $\{\lambda_t\}_{t\in\Tc}$ be the sequencing coverage at different 
generations, then, the observed data are sampled according to 
\beq
d_t \sim \poiss(\lambda_t), \hspace{1in} c_t \sim \bino(d_t,y_t)
\eeq
where the the emission probabilities for a observed tuple  $x_t=\langle d_t, 
c_t\rangle $ is
\beq
	{\bf e}_{i}(x_t) = {d_t \choose c_t} \left(\frac{i}{2n} \right)^{c_t}\left 
	(1- 
	\frac{i}{2n} \right)^{d_t-c_t}.
\eeq
\ignore{The time-series data is represented by the sequence
$\bm{x}=\{x_0,x_1,x_2,\ldots,x_T\}$. We model the dynamic pool-seq
data using an HMM with $2N+1$ states which state $i$ $(0\le i\le
2N)$ corresponds to allele frequency $i/2N$.  Also, we note that the HMM
is a stationary model in that transition and emission distributions do
not change over time and defining these distributions completely
specifies the HMM model. Eqs.~\ref{eq:P1},~\ref{eq:Pt} define
transition probabilities. The probability that state $i$ emits $
x=\langle d, c\rangle $ is given by}


For $1\le t\le T$, let $\alpha_{t,i}$ denote the probability of
emitting $x_1,x_2,\ldots,x_t$ and reaching state $i$ at
$\tau_t$. Then, $\alpha_{t,i}$ can be computed using the
forward-procedure~\cite{durbin1998biological}:
\beq
	\alpha_{t,i} &= \left( \sum_{1\le j\le 2N} 
	\alpha_{t-1,j}\;P^{(\delta_t)}[j,i] \right) Y {\bf 
	e}_{i}(x_t)\;\; \\
	\alpha_t&=\text{Diag}(\alpha_{t-1}) P^{(\delta_t)} Y \bfe(x_t)
	\label{eq:hmm}
\eeq
where $\delta_t=\tau_t-\tau_{t-1}$. The joint likelihood of the
observed data from $R$ independent observations is given by
\begin{equation}
\pr(\{\bm{x}^{(r)}\}|N,n) =	\Lc(N|\{\bm{x}^{(r)}\}, 
n)=\prod_{r=1}^R\Lc(N|\bm{x}^{(r)},n)
	 = 
	\prod_{r=1}^R \sum_i\alpha_{T,i}^{(r)}
	\label{eq:hmmlik}
\end{equation}
where $\bm{x}=\{x_t\}_{t\in \Tc}$. The graphical model and the generative 
process for which data is being generated is depicted in~\ref{fig:1}-B 
and~\ref{proc:arya}, respectively.

Finally, the last step is to find the  $\hN$ in which maximizes the likelihood of 
the all the $M$ variants in whole genome:
\beq
\hN = \underset{N}{\arg \max} \prod_i^M \Lc(N|\{\bm{x}_i^{(r)}\} 
\label{eq:mlen}
\eeq

\subsection{Estimating Selection Parameters}

\paragraph{Likelihood for Selection Model.}
Assume that the site is evolving under selection constraints $s\in
\Rbb$, $h\in \Rbb_+$, where $s$ and $h$ denote selection strength and 
overdominance parameters ,
respectively. By definition, the relative fitness values of genotypes
0$|$0, 0$|$1 and 1$|$1 are given by $w_{00}=1$, $w_{01}=1+hs$ and
$w_{11}=1+s$. Recall that $\nu_t$ denotes the frequency of the site at
time $\tau_t\in {\cal T}$. Then, $\nusp$, the frequency at time
$\tau_{t}+1$ (one generation ahead), can be estimated using: 
\beq 
\hat{\nu}_{t^+} =
\mathbb{E}[\nusp|s,h,\nu_t]&=\frac{w_{11}\nu_t^2 +
  w_{01}\nu_t(1-\nu_t)}{w_{11}\nu^2_t + 2w_{01}\nu_t(1-\nu_t) +
  w_{00}(1-\nu_t)^2}\\
&=\nu_t+\frac{s(h+(1-2h)\nu_t)\nu_t(1-\nu_t)}{1+s\nu_t(2h+(1-2h)\nu_t))}.
  \label{eq:transition}
\eeq
The machinery for computing likelihood of the selection parameters is 
identical to that of population size, except for transition matrices. Hence, here 
we only describe the definition transition matrix $Q_{s,h}$ of the selection 
model.
Let $Q^{(\tau)}_{s,h}[i,j]$ denote the
probability of transition from ${i}/{2N}$ to ${j}/{2N}$ in
$\tau$ generations, then (See~\cite{Ewens2012Mathematical}, Pg.~24, 
Eqn.~$1.58$-$1.59$):
\begin{eqnarray}
  Q^{(1)}_{s,h}[i,j] &=& \pr\left(\nusp=\frac{j}{2N} \left\lvert
      \nu_{t}=\frac{i}{2N};s,h,N \right .\right)={2N \choose j}
  \hat{\nu}_{t^+}^{j} (1-\hat{\nu}_{t^+})^{2N-j}\label{eq:Q1}\\
  Q^{(\tau)}_{s,h} &=& Q^{(\tau-1)}_{s,h}Q^{(1)}_{s,h}\label{eq:Qt}
  \label{eq:mkvs}   
\end{eqnarray}
The maximum likelihood estimates are given by
\beq
\hs,\hh = \underset{s,h}{\arg \max} \prod_i^M \Lc(s,h|\{\bm{x}_i^{(r)},\hN\} 
\label{eq:mlesh}
\eeq

 The parameters in
 Eqs.~\ref{eq:mlen},~\ref{eq:mlesh} are optimized using grid
 search. By broadcasting and vectorizing the grid search operations
 across all variants, the genome scan on millions of polymorphisms can
 be done in significantly smaller time than iterating a numerical
 optimization routine for each variant(see Results and
 \ref{fig:runTime}).
\subsection{Empirical Likelihood Ratio Statistics}
The  likelihood
ratio statistic for testing directional selection, to be computed for each variant, 
is given by
\beq
	H &= -2 \log 
	\left(\frac{\Lc(\hs,0.5|\{\bm{x}^{(r)}\},\hN)}{\Lc(0,0.5|\{\bm{x}^{(r)}\},\hN)}\right)\\
	\label{eq:ELRS}
\eeq

Similarly we can define test statistic for testing if selection is over-dominant 
 \beq
 D &= -2 \log 
 \left(\frac{\Lc(\hs,\hh|\{\bm{x}^{(r)}\},\hN)}{\Lc(\bar{s},0.5|\{\bm{x}^{(r)}\},\hN)}\right),
 \text{ where}
 &\bar{s} = \underset{s}{\arg \max} \prod_i^M 
 \Lc(s,0.5|\{\bm{x}_i^{(r)},\hN\}.
 \eeq

While extending the single-locus WF model to a multiple linked-loci can 
improve the power of the model~\cite{Terhorst2015Multi}, it is 
computationally and statistically expensive to compute exact likelihood.
haplotype resolved data, which pool-seq does not provide. Instead, similar to 
Nielse~\emph{et al}~\cite{nielsen2005genomic}, we calculate  Composite 
Likelihood Ratio score for a genomic region.
\begin{equation}
\Hc = \frac{1}{|L|}\sum_{\ell \in L} H_\ell.
\label{eq:pihmm}
\end{equation}
where $L$ to be a collection of segregating sites
and  $H_\ell$  is the likelihood ratio score based for each variant $\ell$ in $L$.
The optimal value of the hyper-parameter $L$ depends upon a number of factors, 
including, initial frequency of the favored allele, recombination rates, initial 
linkage of the favored allele to its surrounding variation, population size, 
coverage, and time since the 
onset of selection (duration of the experiment). 
However, we provide a heuristic choose size of $L$ for an experiment.

In general, as selection acts locally in the genome, size of $L$ have a direct 
effect on the 
power. For instance, when $L$ is chosen to be a large region (e.g. 
chromosome), power will be degraded since distribution of null and 
alternative $\Hc$ statistics converge together. 
Hence, we choose $L$ to be the largest, such that it provides enough 
discoveries that satisfies experiment's FDR.


\subsection{Hypothesis Testing}
\paragraph{Single-Locus.}
Under neutrality, Wilksâ€™s theorem~\cite{williams2001weighing} states that 
$H\sim\Xc^2_1$, asymptotically, and $p$-values can be computed directly.
However, Feder \emph{et
	al.}~\cite{feder2014Identifying} showed that when the
number of independent samples (replicates) is small, $\Xc^2$ is a crude 
approximation to the true null distribution and underestimates FDR. They 
suggested to compute $p$-values based on the empirical
distribution of statistic on simulations with $\hN$ . Here, take the same 
approach and conduct single 
locus drift simulations starting from 
initial frequencies of the experimental data. Then we sample read counts of the 
derived allele given coverage and allele frequency, see~\ref{proc:arya} for 
step-by-step procedure. 
Test 
statistic is then computed for simulated drifting pool-seq data and $p$-value 
of an 
experimental samples is computed as fraction of null statistics that have a 
higher or equal value than testing statistic.
Finally, we use Storey and 
Tibshirani's method~\cite{storey2003statistical}, to control for False 
Discovery 
Rate in multiple testing.


\paragraph{Regions.}
To compute null distribution of $\Hc$ one should compute $\Hc$ on a set of 
neutral genome-wide neutral simulations. Also, as selection is expected to have 
local effect on the genome, we normalize $\Hc$ with respect to each chromosome 
both in simulated and experimental data in computing $p$-values:
\beq
\Hc_i^*=\frac{\Hc_i-\mu_\Cc}{\sigma_\Cc}, \hspace{0.5in} \forall i \in 
  \Cc
\eeq
where $\mu_\Cc$ and $\sigma_\Cc$ are the chromosome-wise mean and standard 
deviation. $p$-values and FDR can be computed in the same regime as single 
locus. 
After discovering intervals that satisfy FDR requirement, we further select 
those variants within selected intervals that their individual score $H$ is 
significantly using single locus hypothesis testing and then find the 
intersecting genes accordingly.

\ignore{
\paragraph{Precomputing Transition Matrices.}
\comale\ requires a one-time computation of matrices
$Q^{(\tau)}_{s,h}$ for the entire range of $s,h$
values. Precomputation of $121$ transition matrices for
$s\in\{-0.5,-0.45\ldots,0.5 \}$ and $h\in \{0,0.5,\ldots,5\}$ took
less than 3 minutes ($\approx$ 1 second per matrix) on a desktop
computer with a Intel Core i7 CPU and 16GB of RAM.
}
\ignore{
\subsection{Extending Site Frequency Spectrum  based tests for  time series 
data}\label{sec:extending-sfs}
\label{sec:sfs-ts}
The site frequency spectrum (SFS) is a mainstay of tests of neutrality
and selection, and can be computed using pool-seq data (does not
need haplotypes). Following Fu, 1995~\cite{fu1995statistical}, any
linear combination of the site frequencies is an estimate of
$\theta$. However, under non-neutral conditions, different linear
combinations behave differently. Therefore, many popular tests of
neutrality either compute differences of two estimates of $\theta$, or
perform cross-population tests comparing the $\theta$ estimates in two
different
populations~\cite{achaz2009frequency,ronen2013learning,sabeti2007genome}. 

We asked if SFS-based tests could be adapted for time-series data. A
simple approach is to use cross-population SFS tests on the
populations at time $0$ (before onset of selection), and at time
sample $\tau_t$, for each $t$. Evans \emph{et al.}~\cite{evans2007non} developed
diffusion equations for evolution of SFS in time series, but they are
difficult to solve. Instead, we derive a formula for computing $D_t$,
the dynamic of Tajima's $D$ at generation $t$. 
Specifically, for
initial value $D_0$, initial carrier frequency $\nu_0$ and 
selection coefficient $s$:
\begin{equation}
  D_t=D_0-\log(1-\nu_t) \frac{W_0}{\log(2N)} -\nu_t^2 \Pi_0\;,
  \label{eq:tdt}    
\end{equation}
where $W_0$ and $\Pi_0$ are Watterson's and Tajima's estimates of
$\theta$ in the initial generation (\ref{app:td}).  See
\ref{fig:sfsts} for comparison to empirical values 
from
simulations. Similarly, we show (\ref{app:h}), that the
dynamics of Fay and Wu's $H$ statistic~\cite{fay2000hitchhiking} are
directly related to expected value of the of Haplotype Allele
Frequency (HAF) score~\cite{ronen2015predicting}, and can be written
as a function of $\nu_t$ as follows:
\begin{equation}
  NH_t= \theta \nu_t \left(\frac{\nu_t+1}{2} -
    \frac{1}{(1-\nu_t)N+1}\right) + \theta
  (1-\nu_t)\left(\frac{N+1}{2N}-\frac{1}{(1-\nu_t)N+1}\right)
  \label{eq:ht}
\end{equation}	
In both cases, $\nu_t$ itself can be written as a function of $s,t$
(Eq.~\ref{eq:inf-pop}). This allows us to compute 
likelihood
functions $\Lc_S(s; \{D_t\})$ or $\Lc_S(s; \{H_t\})$. Then, a
likelihood ratio, similar to Eqns.~\ref{eq:mcts},~\ref{eq:hmmml}
provides a statistic for detecting selection in each window. 

However, as $\nu_0$ and $D_0$ are often unknown in sampling from
natural populations, we do not directly use
Eqns.~\ref{eq:tdt},~\ref{eq:ht}. Instead, we heuristically aggregate
statistics throughout time to compute a time-series score
(\ref{app:agg}).
}


		


\subsection{Simulations}
We performed extensive simulations using parameters that have been
used for \dmel experimental
evolution~\cite{kofler2013guide}. See also \ref{fig:1}-A 
for
illustration. To implement real world pool-seq experimental evolution, we 
conducted simulations as follows:
\begin{enumerate}[I.]
\item {\bf Creating initial founder line haplotypes.} Using
  \texttt{msms}~\cite{ewing2010msms}, we created neutral populations for $F$
  founding haplotypes with command \texttt{\$./msms
    <F> 1 -t <2$\mu$LNe> -r <2rNeL> <L>}, where $F=200$ is number of
  founder lines, $N_e=10^6$ is effective population size,
  $r=2\times10^{-8}$ is recombination rate, $\mu=2\times 10^{-9}$ is
  mutation rate and $L=50K$ is the window size in base pairs which
  gives $\theta=2\mu N_eL=200$ and $\rho=2N_erL=2000$.
  
\item{\bf Creating initial diploid population.} To simulate
  experimental evolution of diploid organisms, initial haplotypes were
  first cloned to create $F$ diploid homozygotes. Next, each diploid
  individual was cloned $N/F$ times to yield diploid population of
  size $N$.

\item{\bf Forward Simulation.} We used forward simulations for
  evolving populations under selection. We also consider selection
  regimes which the favored allele is chosen from standing
  variation (not \emph{de novo} mutations). Given initial diploid
  population, position of the site under selection, selection strength
  $s$, number of replicates $R=3$, recombination rate
  $r=2\times10^{-8}$ and sampling times $\Tc=\{0,10,20,30,40,50\}$,
  \texttt{simuPop}~\cite{peng2005simupop} was used to perform forward
  simulation and compute allele frequencies for all of the $R$
  replicates.  For hard sweep (respectively, soft sweep) simulations
  we randomly chose a site with initial frequency of $\nu_0=0.005$
  (respectively, $\nu_0=0.1$) to be the favored allele.
\item{\bf Sequencing Simulation.} Give allele frequency trajectories
  we sampled depth of each site identically and independently from
  Poisson($\lambda$), where $\lambda \in \{30,100,300\}$ is the
  coverage for the experiment. Once depth $d$ is drawn for the site
  with frequency $\nu$, the number of reads $c$ carrying the derived
  allele are sampled according to Binomial$(d,\nu)$. For experiments
  with finite depth the tuple $\langle c,d\rangle$ is the input data
  for each site. 
\end{enumerate}
