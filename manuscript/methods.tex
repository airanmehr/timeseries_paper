\section{Materials and Methods}
\label{sec:method}
Consider a diploid population with fixed size of $N$ individuals where
$\nu_t$ denotes allele frequency of the derived allele at generation
$t$. Experimental evolution for $R$ replicates is conducted.  Samples
of $n$ individuals from each replicate are chosen for pooled
sequencing in generations specified by the set $\Tc= \{\tau_i: 0\le
\tau_0<\tau_1,\ldots< \tau_T\}$. To identify the genes and variants
that are responding to selection pressure, we use the following
procedure:
\begin{enumerate}[(i)]
\item {\bf Estimating population size.} The procedure starts by
  estimating the effective population size, $\hN$, under the
  assumption that much of the genome is evolving neutrally.
\item {\bf Estimating selection parameters.} For each polymorphic
  site, selection and overdominance parameters $s,h$ are estimated so
  as to maximize the likelihood of the time series data, given $\hN$.
\item {\bf Computing likelihood statistics.} For each variant, a
  log-odds ratio of the likelihood of selection model ($s>0$) to the
  likelihood of neutral evolution/drift model is computed. Likelihood
  ratios in a genomic region are combined to compute the \comale\
  statistic for the region.
\item {\bf Hypothesis testing.} The \comale\ (or single locus)
  statistics are normalized using a z-score computation. A null
  distribution of the normalized statistic values is computed using a
  set of whole-genome (single locus, respectively) drift simulations
  with population size of $\hN$, and variant starting frequency and
  coverage matching the experimental data. Given the null
  distribution, $p$-values and corresponding False Discovery Rate
  (FDR) are calculated.
\end{enumerate}
These steps are described in detail below.
\subsection{Estimating Population Size}
Methods for estimating population sizes from temporal neutral
evolution data have been
developed~\cite{williamson1999using,anderson2000monte,
  bollback2008estimation, Terhorst2015Multi,jonas2016estimating}.
However, our method explicitly addresses the biases that arise in
pool-seq data. Specifically, we model the variation in sequence
coverage over different locations, and the noise due to sequencing
only a subset of the individuals in the population.  In addition, many
existing
methods~\cite{bollback2008estimation,feder2014Identifying,topa2015gaussian,Terhorst2015Multi}
are designed for large populations, and model frequency as a
continuous quantity. However, we show that smooth approximations may
be inadequate for small populations, low starting frequencies and
sparse sampling (in time) that are typical in experimental evolution
(see Results, \ref{fig:power}A-C, and \ref{fig:markov}). To this end,
we model the Wright-Fisher Markov process for generating pool-seq
data~(\ref{proc:arya}) via a \emph{discrete} HMM (~\ref{fig:1}-B). We
start by computing a likelihood function for the population size given
neutral pool-seq data.


\paragraph{Likelihood for Neutral Model.}
We model the allele frequency counts $2N\vu_t$ as being sampled from a
Binomial distribution. Specifically,
\begin{eqnarray*} 
  \nu_0 &\sim& \pi,\\
  2N\nu_t|\nu_{t-1} &\sim& \bino(2N,\nu_{t-1}) 
\end{eqnarray*}
where $\pi$ is the global distribution of allele frequencies in the
base population. Here we simply assume is $\pi$ is the site frequency
spectrum of fixed sized neutral population~\ref{fig:sfs}\VB{Something
  is wrong with the reffig macro}. Note that $\pi$ may depend on the
demographic history of the founder lines.

To estimate frequency after $\tau$ transitions, it is enough to
specify the $2N\times2N$ transition matrix $P^{(\tau)}$, where
$P^{(\tau)}[i,j]$ denotes probability of change in allele frequency
from ${i}/{2N}$ to ${j}/{2N}$ in $\tau$ generations:
\begin{eqnarray}
  P^{(1)}[i,j] &=& \pr\left(\nu_{t+1}=\frac{j}{2N} \left|
      \nu_{t}=\frac{i}{2N}\right)={2N \choose j} \right.  \nu_{t}^j
  (1-\nu_{t})^{2N-j}, \;\;\label{eq:P1}\\
  P^{(\tau)} &=&   P^{(\tau-1)}P^{(1)} \label{eq:Pt}
\end{eqnarray}
Finally, in typical E\&R experiments, $n<N$ individuals are randomly
selected for sequencing. The {sampled allele frequencies},
$\{y_{t}\}_{t\in\Tc}$, are also Binomially distributed \beq 2ny_{t}
\sim \text{Binomial}(2n,\nu_t) \eeq We introduce the $2N\times2n$
sampling matrix $Y$, where $Y[i,j]$ stores the probability that the
sample allele frequency is ${j}/{2n}$ given that the true allele
frequency is ${i}/{2N}$\VB{One of sample or true allele frequencies
  needs to be $j/2N$}.

\ignore{In the discussion so far, we assumed that the exact allele 
frequencies
are supplied. However, in most cases, allele frequencies are estimated
from genotype data with small sample size or pool-seq
data~\cite{lynch2014population}
(\ref{fig:stateConditional} and 
\ref{fig:trajectoryReal}).
In
pool-seq datasets, the sequencing depth at a site varies for different
replicates, and different time samples
(\ref{fig:depthHetero}). Therefore, filtering low 
coverage
variants can potentially discard useful information.  For instance, in
analyzing \datadm~\cite{orozco2012adaptation,franssen2015patterns}, by 
setting 
minimum read
depth at a site to be 50, allowing the site to be retained only if the
depth for all time points, and all replicates exceeded $50$, the
number of sites to be analyzed would drop from 1,733,121 to
11,232 (\ref{fig:depth}). To account for this
heterogeneity, we extend the Markov chain likelihood in
Eq.~\ref{eq:mkvlik} to a Hidden Markov Model (HMM) for pool-seq data.}

We denote the pool-seq data for that variant as $\{x_t = \langle
c_t,d_t \rangle\}_{t\in\Tc}$ where $d_t, c_t$ represent the read
depth, and the read count of the derived allele, respectively, at time
$\tau_t$. Let $\{\lambda_t\}_{t\in\Tc}$ be the sequencing coverage at
different generations, then, the observed data are sampled according
to 
\begin{equation} d_t \sim \poiss(\lambda_t), \hspace{1in} c_t \sim
\bino(d_t,y_t) 
\end{equation}
The emission probability for a observed tuple $x_t=\langle d_t,
c_t\rangle $ is 
\begin{equation} {\bf e}_{i}(x_t) = {d_t \choose c_t}
\left(\frac{i}{2n} \right)^{c_t}\left (1- \frac{i}{2n}
\right)^{d_t-c_t} .  
\end{equation}
For $1\le t\le T, 1\le j\le 2N$, let $\alpha_{t,j}$ denote the
probability of emitting $x_1,x_2,\ldots,x_t$ and reaching state $j$ at
$\tau_t$. Then, $\alpha_{t}$ can be computed using the
forward-procedure~\cite{durbin1998biological}:
\begin{align}
	\alpha_t^T&=\alpha_{t-1}^T P^{(\delta_t)}\text{diag}(Y\bfe(x_t))
	\label{eq:hmm}
\end{align}
where $\delta_t=\tau_t-\tau_{t-1}$. The joint likelihood of the
observed data from $R$ independent observations is given by
\begin{equation}
\Lc(N|\{\bm{x}^{(r)}\}, 
n)=\prod_{r=1}^R\Lc(N|\bm{x}^{(r)},n)=\pr(\{\bm{x}^{(r)}\}|N,n) =	
	\prod_{r=1}^R \sum_i\alpha_{T,i}^{(r)}
	\label{eq:hmmlik}
\end{equation}
where $\bm{x}=\{x_t\}_{t\in \Tc}$. The graphical model and the generative 
process for which data is being generated is depicted in~\ref{fig:1}-B 
and~\ref{proc:arya}, respectively.

Finally, the last step is to compute an estimate $\hN$ that maximizes
the likelihood of all $M$ variants in whole genome. Let
$\bm{x}_i^{(r)}$ denote the time-series of the $i$-th variant in
replicate $r$. Then,
\begin{equation}
 \hN =
\underset{N}{\arg \max} \prod_{i=1}^M  \prod_{r=1}^R\Lc(N|\bm{x}_i^{(r)})
\label{eq:mlen}
\end{equation}

\subsection{Estimating Selection Parameters}

\paragraph{Likelihood for Selection Model.}
Assume that the site is evolving under selection constraints $s\in
\Rbb$, $h\in \Rbb_+$, where $s$ and $h$ denote selection strength and 
overdominance parameters ,
respectively. By definition, the relative fitness values of genotypes
0$|$0, 0$|$1 and 1$|$1 are given by $w_{00}=1$, $w_{01}=1+hs$ and
$w_{11}=1+s$. Recall that $\nu_t$ denotes the frequency of the site at
time $\tau_t\in {\cal T}$. Then, $\nusp$, the frequency at time
$\tau_{t}+1$ (one generation ahead), can be estimated using: 
\beq 
\hat{\nu}_{t^+} =
\mathbb{E}[\nusp|s,h,\nu_t]&=\frac{w_{11}\nu_t^2 +
  w_{01}\nu_t(1-\nu_t)}{w_{11}\nu^2_t + 2w_{01}\nu_t(1-\nu_t) +
  w_{00}(1-\nu_t)^2}\\
&=\nu_t+\frac{s(h+(1-2h)\nu_t)\nu_t(1-\nu_t)}{1+s\nu_t(2h+(1-2h)\nu_t))}.
  \label{eq:transition}
\eeq
The machinery for computing likelihood of the selection parameters is 
identical to that of population size, except for transition matrices. Hence, here 
we only describe the definition transition matrix $Q_{s,h}$ of the selection 
model.
Let $Q^{(\tau)}_{s,h}[i,j]$ denote the
probability of transition from ${i}/{2N}$ to ${j}/{2N}$ in
$\tau$ generations, then (See~\cite{Ewens2012Mathematical}, Pg.~24, 
Eqn.~$1.58$-$1.59$):
\begin{eqnarray}
  Q^{(1)}_{s,h}[i,j] &=& \pr\left(\nusp=\frac{j}{2N} \left\lvert
      \nu_{t}=\frac{i}{2N};s,h,N \right .\right)={2N \choose j}
  \hat{\nu}_{t^+}^{j} (1-\hat{\nu}_{t^+})^{2N-j}\label{eq:Q1}\\
  Q^{(\tau)}_{s,h} &=& Q^{(\tau-1)}_{s,h}Q^{(1)}_{s,h}\label{eq:Qt}
  \label{eq:mkvs}   
\end{eqnarray}
The maximum likelihood estimates are given by
\beq
\hs,\hh = \underset{s,h}{\arg \max} \prod_{r=1}^R \Lc(s,h|\bm{x}^{(r)},\hN) 
\label{eq:mlesh}
\eeq

Using grid search, we first estimate $N$ (Eq.~\ref{eq:mlen}), and
subsequently, we estimate parameters $s,h$ (Eq.~\ref{eq:mlesh}). By
broadcasting and vectorizing the grid search operations across all
variants, the genome scan on millions of polymorphisms can be done in
significantly smaller time than iterating a numerical optimization
routine for each variant(see Results and \ref{fig:runTime}).
\subsection{Empirical Likelihood Ratio Statistics}
The  likelihood
ratio statistic for testing directional selection, to be computed for each variant, 
is given by
\beq
	H &= -2 \log 
	\left(\frac{\Lc(\bar{s},0.5|\{\bm{x}^{(r)}\},\hN)}{\Lc(0,0.5|\{\bm{x}^{(r)}\},\hN)}\right),\\
	\label{eq:ELRS}
\eeq
where $\bar{s} = \underset{s}{\arg \max} \prod_{r=1}^R 
 \Lc(s,0.5|\bm{x}^{(r)},\hN)$. Similarly we can define a test statistic for testing if selection is over-dominant as:
\beq
 D &= -2 \log 
 \left(\frac{\Lc(\hs,\hh|\bm{x}^{(r)},\hN)}{\Lc(\bar{s},0.5|\bm{x}^{(r)},\hN)}\right).
 \eeq





 While extending the single-locus WF model to a multiple linked-loci
 can improve the power of the model~\cite{Terhorst2015Multi}, it is
 computationally and statistically expensive to compute exact
 likelihood\VB{Please provide plain text from the git repository to
   allow me to correct this}.  haplotype resolved data, which pool-seq
 does not provide. Instead, similar to Nielse~\emph{et
   al}~\cite{nielsen2005genomic}, we calculate Composite Likelihood
 Ratio score for a genomic region.
\begin{equation}
\Hc = \frac{1}{|L|}\sum_{\ell \in L} H_\ell.
\label{eq:pihmm}
\end{equation}
where $L$ is a collection of segregating sites and $H_\ell$ is the
likelihood ratio score based for each variant $\ell$ in $L$.  The
optimal value of the hyper-parameter $L$ depends upon a number of
factors, including initial frequency of the favored allele,
recombination rates, linkage of the favored allele to neighboring
variants, population size, coverage, and time since the onset of
selection (duration of the experiment). However, we provide a
heuristic choose size of $L$ for an experiment \VB{WE cannot use this
  argument. You are saying that L is very dependent on these
  parameters and then you say that it can have large impact on the
  final answer. In that case why should the reviewer allow us to
  choose arbitrary value? We need to say that the answer \emph{does
    not} change when L is chosen in a reasonable range. This needs to
  be discussed and rewritten.}.

In general, as selection acts locally in the genome, size of $L$ have
a direct effect on the power. For instance, when $L$ is chosen to be a
large region (e.g.  chromosome), power will be degraded since
distribution of null and alternative $\Hc$ statistics converge
together.  Hence, we choose $L$ to be the largest, such that it
provides enough discoveries that satisfies experiment's FDR.


\subsection{Hypothesis Testing}
\paragraph{Single-Locus tests.}
Under neutrality, Log-likelihood ratios can be approximated by $\Xc^2$
distribution~\cite{williams2001weighing}, and $p$-values can be
computed directly. However, Feder \emph{et
  al.}~\cite{feder2014Identifying} showed that when the number of
independent samples (replicates) is small, $\Xc^2$ is a crude
approximation to the true null distribution and underestimates
FDR. Following their suggestion, we compute $p$-values based on the
empirical distribution of statistic on simulations using the estimated
population size. (See~\ref{proc:arya} for details).  The empirical
distribution of statistic $H$ is used to compute $p$-values as
fraction of null values that exceed the test score.  Finally, we use
Storey and Tibshirani's method~\cite{storey2003statistical} to control
for False Discovery Rate in multiple testing.


\paragraph{Composite likelihood tests.}
As selection is expected to have local effect on the genome, we
normalize $\Hc$ with respect to each chromosome both in simulated and
experimental data:
\begin{equation} \Hc_i^*=\frac{\Hc_i-\mu_\Cc}{\sigma_\Cc},
\hspace{0.5in} \forall i \in \Cc,
\end{equation} 
where $\mu_\Cc$ and $\sigma_\Cc$ are the mean and standard deviation
of $\Hc$ values in a large (entire chromosome) region. The normalize
$\Hc$ scores are used to compute $p$-values and FDR using the
methodology for single locus analysis. After discovering intervals
that exceed the cut-off for the desired FDR, we further select
variants within selected intervals that have significant individual
scores based on single-locus tests, and identify the genes spanning
those variants.

\ignore{
\paragraph{Precomputing Transition Matrices.}
\comale\ requires a one-time computation of matrices
$Q^{(\tau)}_{s,h}$ for the entire range of $s,h$
values. Precomputation of $121$ transition matrices for
$s\in\{-0.5,-0.45\ldots,0.5 \}$ and $h\in \{0,0.5,\ldots,5\}$ took
less than 3 minutes ($\approx$ 1 second per matrix) on a desktop
computer with a Intel Core i7 CPU and 16GB of RAM.
}
\ignore{
\subsection{Extending Site Frequency Spectrum  based tests for  time series 
data}\label{sec:extending-sfs}
\label{sec:sfs-ts}
The site frequency spectrum (SFS) is a mainstay of tests of neutrality
and selection, and can be computed using pool-seq data (does not
need haplotypes). Following Fu, 1995~\cite{fu1995statistical}, any
linear combination of the site frequencies is an estimate of
$\theta$. However, under non-neutral conditions, different linear
combinations behave differently. Therefore, many popular tests of
neutrality either compute differences of two estimates of $\theta$, or
perform cross-population tests comparing the $\theta$ estimates in two
different
populations~\cite{achaz2009frequency,ronen2013learning,sabeti2007genome}. 

We asked if SFS-based tests could be adapted for time-series data. A
simple approach is to use cross-population SFS tests on the
populations at time $0$ (before onset of selection), and at time
sample $\tau_t$, for each $t$. Evans \emph{et al.}~\cite{evans2007non} developed
diffusion equations for evolution of SFS in time series, but they are
difficult to solve. Instead, we derive a formula for computing $D_t$,
the dynamic of Tajima's $D$ at generation $t$. 
Specifically, for
initial value $D_0$, initial carrier frequency $\nu_0$ and 
selection coefficient $s$:
\begin{equation}
  D_t=D_0-\log(1-\nu_t) \frac{W_0}{\log(2N)} -\nu_t^2 \Pi_0\;,
  \label{eq:tdt}    
\end{equation}
where $W_0$ and $\Pi_0$ are Watterson's and Tajima's estimates of
$\theta$ in the initial generation (\ref{app:td}).  See
\ref{fig:sfsts} for comparison to empirical values 
from
simulations. Similarly, we show (\ref{app:h}), that the
dynamics of Fay and Wu's $H$ statistic~\cite{fay2000hitchhiking} are
directly related to expected value of the of Haplotype Allele
Frequency (HAF) score~\cite{ronen2015predicting}, and can be written
as a function of $\nu_t$ as follows:
\begin{equation}
  NH_t= \theta \nu_t \left(\frac{\nu_t+1}{2} -
    \frac{1}{(1-\nu_t)N+1}\right) + \theta
  (1-\nu_t)\left(\frac{N+1}{2N}-\frac{1}{(1-\nu_t)N+1}\right)
  \label{eq:ht}
\end{equation}	
In both cases, $\nu_t$ itself can be written as a function of $s,t$
(Eq.~\ref{eq:inf-pop}). This allows us to compute 
likelihood
functions $\Lc_S(s; \{D_t\})$ or $\Lc_S(s; \{H_t\})$. Then, a
likelihood ratio, similar to Eqns.~\ref{eq:mcts},~\ref{eq:hmmml}
provides a statistic for detecting selection in each window. 

However, as $\nu_0$ and $D_0$ are often unknown in sampling from
natural populations, we do not directly use
Eqns.~\ref{eq:tdt},~\ref{eq:ht}. Instead, we heuristically aggregate
statistics throughout time to compute a time-series score
(\ref{app:agg}).
}


		


\subsection{Simulations}
We performed extensive simulations using parameters that have been
used for \dmel experimental
evolution~\cite{kofler2013guide}. See also \ref{fig:1}-A 
for
illustration. To implement real world pool-seq experimental evolution, we 
conducted simulations as follows:
\begin{enumerate}[I.]
\item {\bf Creating initial founder line haplotypes.} Using
  \texttt{msms}~\cite{ewing2010msms}, we created neutral populations for $F$
  founding haplotypes with command \texttt{\$./msms
    <F> 1 -t <2$\mu$LNe> -r <2rNeL> <L>}, where $F=200$ is number of
  founder lines, $N_e=10^6$ is effective population size,
  $r=2\times10^{-8}$ is recombination rate, $\mu=2\times 10^{-9}$ is
  mutation rate and $L=50K$ is the window size in base pairs which
  gives $\theta=2\mu N_eL=200$ and $\rho=2N_erL=2000$.
  
\item{\bf Creating initial diploid population.} To simulate
  experimental evolution of diploid organisms, initial haplotypes were
  first cloned to create $F$ diploid homozygotes. Next, each diploid
  individual was cloned $N/F$ times to yield diploid population of
  size $N$.

\item{\bf Forward Simulation.} We used forward simulations for
  evolving populations under selection. We also consider selection
  regimes which the favored allele is chosen from standing
  variation (not \emph{de novo} mutations). Given initial diploid
  population, position of the site under selection, selection strength
  $s$, number of replicates $R=3$, recombination rate
  $r=2\times10^{-8}$ and sampling times $\Tc=\{0,10,20,30,40,50\}$,
  \texttt{simuPop}~\cite{peng2005simupop} was used to perform forward
  simulation and compute allele frequencies for all of the $R$
  replicates.  For hard sweep (respectively, soft sweep) simulations
  we randomly chose a site with initial frequency of $\nu_0=0.005$
  (respectively, $\nu_0=0.1$) to be the favored allele.
\item{\bf Sequencing Simulation.} Give allele frequency trajectories
  we sampled depth of each site identically and independently from
  Poisson($\lambda$), where $\lambda \in \{30,100,300\}$ is the
  coverage for the experiment. Once depth $d$ is drawn for the site
  with frequency $\nu$, the number of reads $c$ carrying the derived
  allele are sampled according to Binomial$(d,\nu)$. For experiments
  with finite depth the tuple $\langle c,d\rangle$ is the input data
  for each site. 
\end{enumerate}
