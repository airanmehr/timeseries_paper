\section{Materials and Methods}
\label{sec:method}

\paragraph{Statistical Procedure.}
To find the genes that are responding to the selection pressure, we
consider a likelihood-based
approach~\cite{bollback2008estimation, 
vitti2013detecting,nielsen2005genomic,topa2015gaussian,Terhorst2015Multi}.
The statistical procedure involves
\begin{enumerate}[(i)]
	\item  {\bf Estimating population size.} The procedure starts by 
	finding 
	maximum likelihood estimate of population size, 
	$\hN$, over the whole genome.
	\item {\bf Estimating selection parameters.} Given $\hN$, 
	maximizing the 
	likelihood of the time series data w.r.t. 
	selection and overdominance parameters $s,h$, for each	
	polymorphism.
	\item {\bf Computing likelihood statistics.} For each variant, it 
	calculates in order to calculate the log-odds 
	ratio of the likelihood of selection model to the likelihood of neutral
	evolution/drift model. Likelihood ratios in a genomic region
	are combined to compute the \comale\ statistic, which is a composite
	likelihood score for the region being under selection. Extremal analysis 
	will provide candidate regions for further functional analysis.
	\item {\bf Hypothesis testing.} The null distribution of likelihood 
	ratio 
	statistics are computed on 
	a set of single locus drift simulations with population size of $\hN$, and 
	configuration (e.g. sequencing coverage) as closely as possible to the EE 
	in which real data generated. Given the null distribution of statistics, 
	$p$-value of the observed likelihood ratios are calculated.
	False Discovery Rate (FDR) correction is applied to the $p$-values 
	to 
	account for multiple testing. The overlapping genes with the 
	variants that 
	satisfy FDR criterion, 
	will be reported for functional analysis or imported to the Gene Set 
	Enrichment Analysis (GSEA).
	\item {\bf Extra analysis.} of estimated parameter could reveal 
	extra 
	information, e.g., overdominance or fixation time, regarding the ongoing 
	selective sweep.
\end{enumerate}
In the subsequent of this section, we outline different steps of the 
statistical procedure.
\subsection{Estimating Population Size}
Estimating population sizes from temporal neutral evolution data has 
been previously studied~\cite{williamson1999using,anderson2000monte, 
bollback2008estimation, Terhorst2015Multi}. Existing methods are well 
designed 
for when 
the allele 
frequencies are computed from a finite sample, that is the 
ascertainment bias 
is uniform over the genome. However, in the case of pool-seq data in 
addition 
to uniform ascertainment bias, 
each variant is sampled at different rate, due to finite sequencing 
coverage. In addition, majority of the state-of-the-art 
models~\cite{bollback2008estimation,feder2014Identifying,topa2015gaussian,Terhorst2015Multi}
model time and state space as continuous variables. However, in our 
experiments, we found that the
smooth approximations is inadequate for small populations, low
starting frequencies and sparse sampling (in time) that are typical in
experimental evolution (see Results, \ref{fig:power}A-C, and 
\ref{fig:markov}). To this end, we use using a discrete-time
discrete-state-space Wright-Fisher Markov chain to model dynamic 
pool-seq data. 
In order to find an estimate of population size, we first need to find 
likelihood of the population size given neutral pool-seq data.


\paragraph{Likelihood for Neutral Model.}
Consider a neutrally evolving diploid population with fixed size of  
$N$ 
individuals where 
$\nu_t$ denotes allele frequency of the derived allele at generation 
$t$.
Experimental evolution for $R$ replicates is conducted so that at 
generations 
 $\Tc= \{\tau_i: 0\le \tau_0<\tau_1,\ldots< \tau_T\}$, $n$ individuals 
 are chosen for sequencing. 
 

 At the highest level, the consecutive allele frequencies of the population in 
 a fixed-size Wright-Fisher model evolves by Binomial sampling
 \beq
 \nu_0 \sim \pi, \hspace{1in} 2N\nu_t|\nu_{t-1} \sim \bino(2N,\nu_{t-1})
 \eeq
where is the marginal distribution of allele frequencies in the base 
population. In general $\pi$ depends on demographic history of the founder 
lines, here we simply assume is $\pi$ is the site frequency spectrum of fixed 
sized neutral population~\ref{fig:sfs}.

To compute distributions after $\tau$ transitions, it is enough to specify the
$2N\times2N$ transition matrix $P^{(\tau)}$, where $P^{(\tau)}[i,j]$ denotes
probability of change in allele frequency from $\frac{i}{2N}$ to
$\frac{j}{2N}$ in $\tau$ generations:
\begin{eqnarray}
  P^{(1)}[i,j] &=& \pr\left(\nu_{t+1}=\frac{j}{2N} \left|
      \nu_{t}=\frac{i}{2N}\right)={2N \choose j} \right.  \nu_{t}^j
  (1-\nu_{t})^{2N-j}, \;\;\label{eq:P1}\\
  P^{(\tau)} &=&   P^{(\tau-1)}P^{(1)} \label{eq:Pt}
\end{eqnarray}
\ignore{
and $c_t$,$d_\tau$ be 
the 
number of reads of alternate allele and total number of read at a 
locus for 
each replicate, then the graphical model~\ref{fig:GM} completely 
specifies the 
model for a single locus. Note that, for each replicate the only 
observed 
variables are $\bfc=\{c_0,\ldots,c_T\}$ and $\bfd=\{d_0,\ldots,d_T\}$, 
shaded 
nodes in~\ref{fig:GM}. The generative model is}
As at each generation $n$ out of $N$  individuals are randomly selected 
for sequencing, The \emph{sampled allele frequencies}, $\{y_{t}\}_{t\in\Tc}$, 
are also Binomially distributed
\beq
2ny_{t} \sim \text{Binomial}(2n,\nu_t)
\eeq
We introduce the $2N\times2n$ sampling matrix $Y$, where $Y[i,j]$ stores the 
probability that the sample allele frequency is $\frac{i}{2n}$ given that the 
true 
allele frequency is $\frac{i}{2N}$.

\ignore{In the discussion so far, we assumed that the exact allele 
frequencies
are supplied. However, in most cases, allele frequencies are estimated
from genotype data with small sample size or pool-seq
data~\cite{lynch2014population}
(\ref{fig:stateConditional} and 
\ref{fig:trajectoryReal}).
In
pool-seq datasets, the sequencing depth at a site varies for different
replicates, and different time samples
(\ref{fig:depthHetero}). Therefore, filtering low 
coverage
variants can potentially discard useful information.  For instance, in
analyzing \datadm~\cite{orozco2012adaptation,franssen2015patterns}, by 
setting 
minimum read
depth at a site to be 50, allowing the site to be retained only if the
depth for all time points, and all replicates exceeded $50$, the
number of sites to be analyzed would drop from 1,733,121 to
11,232 (\ref{fig:depth}). To account for this
heterogeneity, we extend the Markov chain likelihood in
Eq.~\ref{eq:mkvlik} to a Hidden Markov Model (HMM) for pool-seq data.}

We denote the pool-seq data for that variant as $\{x_t =
\langle c_t,d_t \rangle\}_{t\in\Tc}$ where $d_t, c_t$ represent the read depth,
and the read count of the derived allele, respectively, at time
$\tau_t$. Let $\{\lambda_t\}_{t\in\Tc}$ be the sequencing coverage at different 
generations, then, the observed data are sampled according to 
\beq
d_t \sim \poiss(\lambda_t), \hspace{1in} c_t \sim \bino(d_t,y_t)
\eeq
where the the emission prbabilities for a observed tuple  $x_t=\langle d_t, 
c_t\rangle $ is
\beq
	{\bf e}_{i}(x_t) = {d_t \choose c_t} \left(\frac{i}{2n} \right)^{c_t}\left 
	(1- 
	\frac{i}{2n} \right)^{d_t-c_t}.
\eeq
\ignore{The time-series data is represented by the sequence
$\bm{x}=\{x_0,x_1,x_2,\ldots,x_T\}$. We model the dynamic pool-seq
data using an HMM with $2N+1$ states which state $i$ $(0\le i\le
2N)$ corresponds to allele frequency $i/2N$.  Also, we note that the HMM
is a stationary model in that transition and emission distributions do
not change over time and defining these distributions completely
specifies the HMM model. Eqs.~\ref{eq:P1},~\ref{eq:Pt} define
transition probabilities. The probability that state $i$ emits $
x=\langle d, c\rangle $ is given by}


For $1\le t\le T$, let $\alpha_{t,i}$ denote the probability of
emitting $x_1,x_2,\ldots,x_t$ and reaching state $i$ at
$\tau_t$. Then, $\alpha_{t,i}$ can be computed using the
forward-procedure~\cite{durbin1998biological}:
\beq
	\alpha_{t,i} &= \left( \sum_{1\le j\le 2N} 
	\alpha_{t-1,j}\;P^{(\delta_t)}[j,i] \right) Y {\bf 
	e}_{i}(x_t)\;\; \\
	\alpha_t&=\text{Diag}(\alpha_{t-1}) P^{(\delta_t)} Y \bfe(x_t)
	\label{eq:hmm}
\eeq
where $\delta_t=\tau_t-\tau_{t-1}$. The joint likelihood of the
observed data from $R$ independent observations is given by
\begin{equation}
\pr(\{\bm{x}^{(r)}\}|N,n) =	\Lc(N|\{\bm{x}^{(r)}\}, 
n)=\prod_{r=1}^R\Lc(N|\bm{x}^{(r)},n)
	 = 
	\prod_{r=1}^R \sum_i\alpha_{T,i}^{(r)}\;\;.
	\label{eq:hmmlik}
\end{equation}
where $\bm{x}=\{x_t\}_{t\in \Tc}$. The graphical model and the generative 
process for which data is being generated is depicted in~\ref{fig:GM}.

Finally, the last step is to find the  $\hN$ in which maximizes the likelihood of 
the all variants in whole genome:
\beq
\hN = \underset{N}{\arg \max} \prod_i^M \Lc(N|\{\bm{x}_i^{(r)}\} 
\label{eq:mlen}
\eeq

\subsection{Estimating Selection Parameters}

\paragraph{Likelihood for Selection Model.}
Assume that the site is evolving under selection constraints $s\in
\Rbb$, $h\in \Rbb_+$, where $s$ and $h$ denote selection strength and 
overdominance parameters ,
respectively. By definition, the relative fitness values of genotypes
0$|$0, 0$|$1 and 1$|$1 are given by $w_{00}=1$, $w_{01}=1+hs$ and
$w_{11}=1+s$. Recall that $\nu_t$ denotes the frequency of the site at
time $\tau_t\in {\cal T}$. Then, $\nusp$, the frequency at time
$\tau_{t}+1$ (one generation ahead), can be estimated using: \beq 
\hat{\nu}_{t^+} =
\mathbb{E}[\nusp|s,h,\nu_t]&=\frac{w_{11}\nu_t^2 +
  w_{01}\nu_t(1-\nu_t)}{w_{11}\nu^2_t + 2w_{01}\nu_t(1-\nu_t) +
  w_{00}(1-\nu_t)^2}\\
&=\nu_t+\frac{s(h+(1-2h)\nu_t)\nu_t(1-\nu_t)}{1+s\nu_t(2h+(1-2h)\nu_t))}.
  \label{eq:transition}
\eeq
The machinery for computing likelihood of the selection parameters is 
identical to that of population size, except for transition matrices. Hence, here 
we only introduce the definition transition matrix $Q_{s,h}$ of the selection 
model.
Let $Q^{(\tau)}_{s,h}[i,j]$ denote the
probability of transition from $\frac{i}{2N}$ to $\frac{j}{2N}$ in
$\tau$ generations, then (See~\cite{Ewens2012Mathematical}, Pg.~24, 
Eqn.~$1.58$-$1.59$):
\begin{eqnarray}
  Q^{(1)}_{s,h}[i,j] &=& \pr\left(\nusp=\frac{j}{2N} \left\lvert
      \nu_{t}=\frac{i}{2N};s,h,N \right .\right)={2N \choose j}
  \hat{\nu}_{t^+}^{j} (1-\hat{\nu}_{t^+})^{2N-j}\label{eq:Q1}\\
  Q^{(\tau)}_{s,h} &=& Q^{(\tau-1)}_{s,h}Q^{(1)}_{s,h}\label{eq:Qt}
  \label{eq:mkvs}   
\end{eqnarray}
The maximum likelihood estimates are given by
\beq
\hs,\hh = \underset{s,h}{\arg \max} \prod_i^M \Lc(s,h|\{\bm{x}_i^{(r)},\hN\} 
\label{eq:mlesh}
\eeq

 The parameters in
 Eqs.~\ref{eq:mlen},~\ref{eq:mlesh} are optimized using grid
 search. By broadcasting and vectorizing the grid search operations
 across all variants, the genome scan on millions of polymorphisms can
 be done in significantly smaller time than iterating a numerical
 optimization routine for each variant(see Results and
 \ref{fig:runTime}).
\subsection{Empirical Likelihood Ratio Statistics}

Similar to Eq.~\ref{eq:mcts}, let $\hat{s},\hat{h}$ denote the
parameters that maximize likelihood. The corresponding likelihood
ratio statistic for each variant of pool-seq data is given by
\beq
	H &= -2 \log 
	\left(\frac{\Lc(\hs,\hh|\{\bm{x}^{(r)}\},\hN)}{\Lc(0,0.5|\{\bm{x}^{(r)}\},\hN)}\right)\\
	\label{eq:ELRS}
\eeq

\paragraph{Composite Likelihood Ratio.}
In general, the favored allele can be in linkage disequilibrium with
some of its surrounding variation. The linked-loci hitchhike and share
similar dynamics with the favored allele~\cite{elyashiv2016genomic}.  Some
models such as
multi-locus Gaussian process~\cite{Terhorst2015Multi} incorporate
these associations by modeling linkage and recombination
explicitly. However, these approaches are computationally
expensive. Moreover, linkage computations are infeasible without
haplotype resolved data, which pool-seq does not provide. Instead, we
work with a simpler Composite Likelihood Ratio
(CLR)~\cite{nielsen2005genomic,williamson2007localizing} computation
to combine the individual scores of all variants into a composite
score.


Consider a genomic region $L$ to be a collection of segregating sites
with little or no recombination between sites and the favored
allele. This scenario holds when the starting frequency of the favored
allele is not high and the region is small. Let  $H_\ell$ denote the likelihood 
ratio score based for each site $\ell$ in $L$. The CLR is computed by 
averaging scores of all the variants
withing the testing region
\begin{equation}
 \Hc = \frac{1}{|L|}\sum_{\ell \in L} H_\ell.
\label{eq:pihmm}
\end{equation}


\subsection{Hypothesis Testing}
\paragraph{$p$-value Computation.}
By Wilksâ€™ theorem~\cite{williams2001weighing}, the likelihood
ratio statistic (Eq.~\ref{eq:ELRS}) is asymptotically distributed
according to $\Xc^2_k$, where $k$ is the difference between dimensions of 
parameters of alternative and null model. Feder \emph{et
	al.}~\cite{feder2014Identifying} showed that the empirical
distribution of statistic on simulations with $\hN$ provide more accurate 
$p$-values than $\Xc^2$ when the
number of independent samples (replicates) is small. 
However, the simulations for composite statistics require initial haplotypes of 
the founder population, local recombination rates and massive computational 
resources. Hence we use $\Xc^2$ for testing composite statistic $\Hc$ and 
simulations for polymorphism statistic.

Specifically, for composite statistics, we make a simplifying assumption that 
is all the statistics ${H_\ell}$ are independent, then
\beq
H_\ell \sim \Xc^2_2 \ \ \Longrightarrow \ \ \sum_{\ell \in L} H_\ell \sim 
\Xc^2_{2L}
\ \ \Longleftrightarrow \ \ \Hc \sim \text{Gamma}(\alpha=L,\theta=2/L)
\eeq

To create single  locus simulations, we repeat the generative process for 
$N=\hN$ outlined 
in the~\ref{fig:GM} for every variant in the real data, with only difference that 
$\nu_0$ and $\{\bm d^{(r)}\}$ are given from the real data.
\paragraph{Correcting for multiple testing}
To control for False Discovery Rate in multiple testing, we by

\paragraph{Enrichment Analysis}

\subsection{Extra Analysis}

\paragraph{Site-identification.} 
Once a genomic region is classified to be under selection by $\Hc$ or $\Mc$ 
statistic,
individual variant scores ($M$ or $H$) in a region are ranked to predict the
favored site. In general, identifying the favored site in pool-seq
data is difficult~\cite{tobler2014massive}, due to extensive span of
hitchhikers in an ongoing sweep (see \ref{app:ld} for 
more
detail). In our analysis of the \dmel EE data, we identify a set of
``candidate'' variants whose scores exceed a False Discovery Rate
threshold based on the distribution of \comale\ scores on negative
controls.

\paragraph{Overdominance.}
\label{sec:regression}
 The value of the overdominance 
 parameter can
reveal if the favored allele
is overdominant, repressive or dominant~\cite{gillespie2010population} 
 (See \ref{tab:h}, \ref{fig:dominance} and \ref{fig:dir-bal}). The test statistic for 
 such hypothesis test is
\beq
D &= -2 \log 
\left(\frac{\Lc(\hs,\hh|\{\bm{x}^{(r)}\},\hN)}{\Lc(\bar{s},0.5|\{\bm{x}^{(r)}\},\hN)}\right),
\text{ where}
&\bar{s} = \underset{s}{\arg \max} \prod_i^M 
\Lc(s,0.5|\{\bm{x}_i^{(r)},\hN\}.
\eeq

\begin{table}
	\centering
	\caption{\bf Overdominance parameter values and their implications.}
	\begin{tabular}{l|c}
		Value & Condition\\
		\hline
		$h=0$ & recessive adaptive allele\\
		$h=0.5$ & directional selection\\
		$h=1$&	dominant adaptive allele	\\
		$h>1$ &overdominance
	\end{tabular}\label{tab:h}
\end{table}


\paragraph{Precomputing Transition Matrices.}
\comale\ requires a one-time computation of matrices
$Q^{(\tau)}_{s,h}$ for the entire range of $s,h$
values. Precomputation of $909$ transition matrices for
$s\in\{-0.5,-0.49,\ldots,0.5 \}$ and $h\in \{0,0.25,\ldots,2\}$ took
less that 15 minutes ($\approx$ 1 second per matrix) on a desktop
computer with a Core i7 CPU and 16GB of RAM.

\ignore{
\subsection{Extending Site Frequency Spectrum  based tests for  time series 
data}\label{sec:extending-sfs}
\label{sec:sfs-ts}
The site frequency spectrum (SFS) is a mainstay of tests of neutrality
and selection, and can be computed using pool-seq data (does not
need haplotypes). Following Fu, 1995~\cite{fu1995statistical}, any
linear combination of the site frequencies is an estimate of
$\theta$. However, under non-neutral conditions, different linear
combinations behave differently. Therefore, many popular tests of
neutrality either compute differences of two estimates of $\theta$, or
perform cross-population tests comparing the $\theta$ estimates in two
different
populations~\cite{achaz2009frequency,ronen2013learning,sabeti2007genome}. 

We asked if SFS-based tests could be adapted for time-series data. A
simple approach is to use cross-population SFS tests on the
populations at time $0$ (before onset of selection), and at time
sample $\tau_t$, for each $t$. Evans \emph{et al.}~\cite{evans2007non} developed
diffusion equations for evolution of SFS in time series, but they are
difficult to solve. Instead, we derive a formula for computing $D_t$,
the dynamic of Tajima's $D$ at generation $t$. 
Specifically, for
initial value $D_0$, initial carrier frequency $\nu_0$ and 
selection coefficient $s$:
\begin{equation}
  D_t=D_0-\log(1-\nu_t) \frac{W_0}{\log(2N)} -\nu_t^2 \Pi_0\;,
  \label{eq:tdt}    
\end{equation}
where $W_0$ and $\Pi_0$ are Watterson's and Tajima's estimates of
$\theta$ in the initial generation (\ref{app:td}).  See
\ref{fig:sfsts} for comparison to empirical values 
from
simulations. Similarly, we show (\ref{app:h}), that the
dynamics of Fay and Wu's $H$ statistic~\cite{fay2000hitchhiking} are
directly related to expected value of the of Haplotype Allele
Frequency (HAF) score~\cite{ronen2015predicting}, and can be written
as a function of $\nu_t$ as follows:
\begin{equation}
  NH_t= \theta \nu_t \left(\frac{\nu_t+1}{2} -
    \frac{1}{(1-\nu_t)N+1}\right) + \theta
  (1-\nu_t)\left(\frac{N+1}{2N}-\frac{1}{(1-\nu_t)N+1}\right)
  \label{eq:ht}
\end{equation}	
In both cases, $\nu_t$ itself can be written as a function of $s,t$
(Eq.~\ref{eq:inf-pop}). This allows us to compute 
likelihood
functions $\Lc_S(s; \{D_t\})$ or $\Lc_S(s; \{H_t\})$. Then, a
likelihood ratio, similar to Eqns.~\ref{eq:mcts},~\ref{eq:hmmml}
provides a statistic for detecting selection in each window. 

However, as $\nu_0$ and $D_0$ are often unknown in sampling from
natural populations, we do not directly use
Eqns.~\ref{eq:tdt},~\ref{eq:ht}. Instead, we heuristically aggregate
statistics throughout time to compute a time-series score
(\ref{app:agg}).
}


		


\subsection{Simulations}
We performed extensive simulations using parameters that have been
used for \dmel experimental
evolution~\cite{kofler2013guide}. See also \ref{fig:ee} 
for
illustration. To implement real world pool-seq experimental evolution, we 
conducted simulations as follows:
\begin{enumerate}[I.]
\item {\bf Creating initial founder line haplotypes.} Using
  \texttt{msms}~\cite{ewing2010msms}, we created neutral populations for $F$
  founding haplotypes with command \texttt{\$./msms
    <F> 1 -t <2$\mu$LNe> -r <2rNeL> <L>}, where $F=200$ is number of
  founder lines, $N_e=10^6$ is effective population size,
  $r=2\times10^{-8}$ is recombination rate, $\mu=2\times 10^{-9}$ is
  mutation rate and $L=50K$ is the window size in base pairs which
  gives $\theta=2\mu N_eL=200$ and $\rho=2N_erL=2000$.
  
\item{\bf Creating initial diploid population.} To simulate
  experimental evolution of diploid organisms, initial haplotypes were
  first cloned to create $F$ diploid homozygotes. Next, each diploid
  individual was cloned $N/F$ times to yield diploid population of
  size $N$.

\item{\bf Forward Simulation.} We used forward simulations for
  evolving populations under selection. We also consider selection
  regimes which the favored allele is chosen from standing
  variation (not \emph{de novo} mutations). Given initial diploid
  population, position of the site under selection, selection strength
  $s$, number of replicates $R=3$, recombination rate
  $r=2\times10^{-8}$ and sampling times $\Tc=\{0,10,20,30,40,50\}$,
  \texttt{simuPop}~\cite{peng2005simupop} was used to perform forward
  simulation and compute allele frequencies for all of the $R$
  replicates.  For hard sweep (respectively, soft sweep) simulations
  we randomly chose a site with initial frequency of $\nu_0=0.005$
  (respectively, $\nu_0=0.1$) to be the favored allele.
\item{\bf Sequencing Simulation.} Give allele frequency trajectories
  we sampled depth of each site identically and independently from
  Poisson($\lambda$), where $\lambda \in \{30,100,300,\infty\}$ is the
  coverage for the experiment. Once depth $d$ is drawn for the site
  with frequency $\nu$, the number of reads $c$ carrying the derived
  allele are sampled according to Binomial$(d,\nu)$. For experiments
  with finite depth the tuple $\langle c,d\rangle$ is the input data
  for each site. Infinite depth experiments refer to the case, where
  the true allele frequency is provided and Markov and HMM likelihood
  computations give identical results.
\end{enumerate}
