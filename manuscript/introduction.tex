\section{Introduction}
Natural selection is a key force in evolution, and a mechanism by
which populations can adapt to external `selection'
constraints. Examples of adaptation abound in the natural 
world~\cite{going2016fan},
including for example, classic examples like lactose tolerance in
Northern Europeans~\cite{bersaglieri2004genetic}, human adaptation to
high altitudes~\cite{yi2010sequencing,simonson2010genetic}, but also
drug resistance in pests~\cite{daborn2001ddt},
HIV~\cite{Feder2016More},
cancer~\cite{gottesman2002mechanisms,zahreddine2013mechanisms},
malarial parasite~\cite{ariey2014molecular,nair2007recurrent}, and
other antibiotic resistance~\cite{spellberg2008epidemic}. In these
examples, understanding the genetic basis of adaptation can provide
actionable information, underscoring the importance of the problem.


Experimental evolution refers to the study of the evolutionary
processes of a model organism in a controlled
\cite{hegreness2006equivalence,lang2013pervasive,orozco2012adaptation,
  lang2011genetic,barrick2009genome,bollback2007clonal,oz2014strength}
or natural
\cite{maldarelli2013hiv,reid2011new,denef2012situ,winters2012development,
  daniels2013genetic,barrett2008natural,bergland2014genomic}
environment. Recent advances in whole genome sequencing have enabled
us to sequence populations at a reasonable cost even for large
genomes. Perhaps more important for experimental evolution studies, we
can now evolve and resequence (E\&R) multiple replicates of a population to
obtain \emph{longitudinal time-series data}, in order to investigate
the dynamics of evolution at molecular level.  Although constraints
such as small sizes, limited timescales, and oversimplified
laboratory environments may limit the interpretation of E\&R results,
these studies are increasingly being used to test a wide range of
hypotheses~\cite{kawecki2012experimental} and have been shown to be
more predictive than static data analysis
\cite{boyko2008assessing,desai2008polymorphism,sawyer1992population}.
In particular, longitudinal E\&R data is being used to estimate model
parameters including population
size~\cite{williamson1999using,wang2001pseudo,pollak1983new,waples1989generalized,
  Terhorst2015Multi, jonas2016estimating}, strength of
selection~\cite{mathieson2013estimating,illingworth2011distinguishing,Terhorst2015Multi,
  bollback2008estimation,illingworth2012quantifying,malaspinas2012estimating,
  steinrucken2014novel}, allele age~\cite{malaspinas2012estimating}
recombination rate~\cite{Terhorst2015Multi}, mutation
rate~\cite{Barrick2013Genome, Terhorst2015Multi}, quantitative trait
loci~\cite{baldwin2014power} and for tests of neutrality
hypotheses~\cite{feder2014Identifying,Terhorst2015Multi,burke2010genome,bergland2014genomic}.


While objectives, designs and organisms of E\&R studies can be entirely
different~\cite{Barrick2013Genome,schlotterer2015combining}, here we
restrict our attention to the adaptive evolution of multi-cellular
sexual organisms.  For simplicity, we assume fixed population size,
and for the most part, positive single locus selection (only one
favored mutation). This regime has been considered earlier, typically
with \dmel as the model organism of choice, to identify adaptive genes
in longevity and aging ~\cite{burke2010genome,remolina2012genomic}
(600 generations), courtship song~\cite{turner2011population} (100
generations), hypoxia tolerance~\cite{zhou2011experimental} (200
generations), adaptation to new laboratory
environments~\cite{orozco2012adaptation,franssen2015patterns} (59
generations), egg size~\cite{jha2015whole} (40 generations), C virus
resistance~\cite{martins2014host} (20 generations), and
dark-fly~\cite{izutsu2015dynamics} (49 generations).


The task of identifying genetic adaptation can be addressed at
different levels of specificity. At the coarsest level, identification
could simply refer to deciding whether some genomic region (or a gene)
is under selection or not. In the following, we refer to this task as
\emph{detection}. In contrast, the task of \emph{site-identification}
corresponds to the process of finding the favored mutation/allele at
nucleotide level. Finally, \emph{estimation of model parameters}, such
as strength of selection and overdominance at the site, can provide a
comprehensive description of the selection process.

\ignore{A wide range of computational methods~\cite{vitti2013detecting} have
been developed to detect regions under positive selection. A majority
of the existing methods focus on static data analysis; analysis of a
single sample of the population at a specific time, either during the
sweep, or subsequent to fixation of the favored allele. Static
analysis is focused on reduction in genetic
diversity~\cite{tajima1989statistical,fay2000hitchhiking,ronen2013learning,garud2015recent}
shift in allele-frequencies, prevalence of long
haplotypes~\cite{sabeti2006positive,vitti2013detecting}, population
differentiation~\cite{holsinger2009genetics,burke2010genome,gunther2013robust}
in
multiple-population data and others. Many existing methods use the
Site Frequency Spectrum (SFS, see \ref{fig:sfs}) to
identify departure from neutrality. Classical examples including
Tajima's \emph{D}~\cite{tajima1989statistical}, Fay and Wu's
\emph{H}~\cite{fay2000hitchhiking}, Composite Likelihood
Ratio~\cite{nielsen2005genomic}, were all shown to be weighted linear
combinations of the SFS values~\cite{achaz2009frequency}.  While
successful, these methods are prone to both, false
negatives~\cite{messer2013population}, and also false-discoveries due
to confounding factors such as demography, including bottleneck and
population expansions, and ascertainment bias ~\cite{ptak2002evidence,
  ramos2002statistical,akey2009constructing,
  nielsen2003correcting,messer2013population}. Nevertheless, SFS based
tests continue to be used successfully, often in combination with
other tests~\cite{akey2009constructing,vitti2013detecting}. One of the
contributions of this paper is the extension of SFS based methods to
analyze time-series data, and the identification of selection regimes
where these methods perform well.}

In an effort for analyzing E\&R selection experiments many authors chose to
adopt existing tests that originally used for static data, for scanning dynamic 
data with two time-points. For instance, Zhu
\emph{et al.}~\cite{zhou2011experimental} used the ratio of the estimated
population size of case and control populations to compute test
statistic for each genomic region. Burke \emph{et
 al.}~\cite{burke2010genome} applied Fisher exact test to the last
observation of data on case and control 
populations. Orozco-terWengel
\emph{et al.}~\cite{orozco2012adaptation} used the
Cochran-Mantel-Haenszel (CMH) test~\cite{agresti2011categorical} to
detect SNPs whose read counts change consistently across all
replicates of two time-point data. Turner \emph{et
 al.}~\cite{turner2011population} proposed the diffStat statistic to
test whether the change in allele frequencies of two populations
deviate from the distribution of change in allele frequencies of two
drifting populations. Bergland \emph{et
 al.}~\cite{bergland2014genomic} applied $F_{st}$ to populations
throughout time to signify their differentiation from ancestral (two
time-point data) as well as geographically different populations. Jha
\emph{et al.}~\cite{jha2015whole} computed test statistic of
generalized linear-mixed model directly from read counts. 

Early \emph{direct} methods for analyzing time-series data
devoted to estimate population size in neutral populations 
~\cite{williamson1999using,anderson2000monte,beaumont2003estimation,berthier2002likelihood,wang2001pseudo},
 using statistical (HMM) and population genetics (Coalescent) models. 
The first effort to properly address the problem of parameter estimation in time 
series 
selection data was done by Bollback \emph{et 
	al.}~\cite{bollback2008estimation}.
 They provided a diffusion approximation 
	to
the continues Wright Fisher Markov process and estimated $s$
numerically for large population sizes.
Malaspinas \emph{et 
al.}~\cite{malaspinas2012estimating} extended Bollback 
et al.â€™s 
method to estimate allele age in ancient-DNA (aDNA).
Steinr\"{u}cken and Song~\cite{steinrucken2014novel} proposed a general 
diploid selection model which 
takes into account of dominance of the favored allele and approximates 
likelihood analytically.
Mathieson and McVean~\cite{mathieson2013estimating} adopted HMMs to 
structured populations and estimated parameters using an Expectation 
Maximization (EM) procedure, on discretized allele frequency.
Feder \emph{et al.}~\cite{feder2014Identifying}
modeled increments in allele frequency with a Brownian motion process,  
proposed the Frequency
Increment Test (FIT). More recently, Topa \emph{et
  al.}~\cite{topa2015gaussian} proposed a Gaussian Process (GP) for
modeling single-locus time-series pool-seq data. Terhorst \emph{et
  al.}~\cite{Terhorst2015Multi} extended GP to compute joint
likelihood of multiple loci under null and alternative hypotheses.
Recently, schraiber \emph{et al.}~\cite{schraiber2016bayesian} proposed a 
Bayesian framework to estimate 
parameters using Monte Carlo Markov chain sampling.


While existing methods have been successfully applied to their corresponding 
application, they make several key assumptions which does not hold in many 
of E\&R studies.
First, they assume that the underlying population size is large, if not infinity,
(See Table 3 in \cite{malaspinas2016methods} for illustration). As a result,  
they provide continues state models for dynamics of allele frequencies.
In addition, most these methods, originally designed to process wide time 
spans, i.e. aDNA studies. Also, they assume input data is allele 
frequency or at least is \emph{sampled} allele frequency, where ascertainment 
bias is uniform along genome.

This manuscript, explicitly posits a ``small-population-size" assumption on 
Williamson~\emph{et al.}~\cite{williamson1999using} and Bollback~\emph{et 
al.}'s model~\cite{bollback2008estimation}, and as a consequence, the 
resulting model become a discrete state (frequency) model. 
We show that for small population sizes discrete models can compute 
likelihood exactly, which make a difference in the statistical performance, 
especially in short term 
experiments. Additionally, we add another level of sampling-noise to the 
traditional HMM, which models heterogeneous ascertainment bias due to 
uneven coverages among variants. We show for a wide range of parameters
that \comale\ provides higher power for detecting selection, is robust
to ascertainment bias due to coverage heterogeneity, estimates model
parameters consistently, and localizes favored allele more accurately
compared to the state-of-the-art methods, while being computationally 
efficient.
